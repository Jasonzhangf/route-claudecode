/**
 * vLLM Server Compatibility Module
 *
 * vLLM OpenAIå…¼å®¹æ€§æ¨¡å—
 * æŒ‰ç…§RCC v4.0æ¶æ„è§„èŒƒå®ç°
 *
 * @author Jason Zhang
 */

import { BasePipelineModule } from '../base-pipeline-module';
import { OpenAI } from 'openai';
import { PIPELINE_ERROR_MESSAGES } from '../../constants/src/pipeline-constants';

/**
 * vLLMé…ç½®æ¥å£
 */
export interface VLLMCompatibilityConfig {
  baseUrl: string;
  apiKey?: string;
  timeout: number;
  maxRetries: number;
  retryDelay: number;
  models: string[];
  features: {
    chat: boolean;
    streaming: boolean;
    parallel: boolean;
  };
  vllmSpecific: {
    engineArgs?: Record<string, any>;
    modelPath?: string;
    tensorParallelSize?: number;
    gpuMemoryUtilization?: number;
  };
}

/**
 * æ ‡å‡†è¯·æ±‚æ¥å£
 */
export interface StandardRequest {
  model: string;
  messages: Array<{
    role: 'system' | 'user' | 'assistant' | 'tool';
    content: string;
    name?: string;
    tool_call_id?: string;
  }>;
  max_tokens?: number;
  temperature?: number;
  top_p?: number;
  frequency_penalty?: number;
  presence_penalty?: number;
  stop?: string | string[];
  stream?: boolean;
}

/**
 * vLLMç‰¹å®šè¯·æ±‚æ ¼å¼
 */
export interface VLLMRequest extends StandardRequest {
  // vLLMç‰¹å®šå‚æ•°
  top_k?: number;
  repetition_penalty?: number;
  length_penalty?: number;
  min_length?: number;
  use_beam_search?: boolean;
  best_of?: number;
  logprobs?: number;
  prompt_logprobs?: number;
  skip_special_tokens?: boolean;
  spaces_between_special_tokens?: boolean;
}

/**
 * æ ‡å‡†å“åº”æ¥å£
 */
export interface StandardResponse {
  id: string;
  object: string;
  created: number;
  model: string;
  choices: Array<{
    index: number;
    message: {
      role: 'assistant';
      content?: string;
    };
    finish_reason: 'stop' | 'length' | 'content_filter';
    logprobs?: any;
  }>;
  usage: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}

/**
 * vLLMå…¼å®¹æ€§æ¨¡å—
 */
export class VLLMCompatibilityModule extends BasePipelineModule {
  private readonly config: VLLMCompatibilityConfig;
  private openaiClient: OpenAI;

  constructor(config: VLLMCompatibilityConfig) {
    super('vllm-compatibility', 'vLLM Compatibility Module', 'server-compatibility' as any, '1.0.0');

    this.config = config;

    // ä½¿ç”¨OpenAI SDKè¿æ¥vLLM
    this.openaiClient = new OpenAI({
      baseURL: config.baseUrl,
      apiKey: config.apiKey || 'vllm', // vLLMé€šå¸¸ä¸éœ€è¦APIå¯†é’¥
      timeout: config.timeout,
    });

    console.log(`ğŸš€ åˆå§‹åŒ–vLLMå…¼å®¹æ¨¡å—: ${config.baseUrl}`);
  }

  /**
   * å¯åŠ¨æ¨¡å—
   */
  protected async doStart(): Promise<void> {
    await this.testConnection();
    console.log(`âœ… vLLMå…¼å®¹æ¨¡å—å¯åŠ¨æˆåŠŸ`);
  }

  /**
   * å¤„ç†è¯·æ±‚
   */
  protected async doProcess(input: StandardRequest): Promise<StandardResponse> {
    const startTime = Date.now();
    console.log(`ğŸš€ vLLMå…¼å®¹å¤„ç†: ${input.model}`);

    try {
      // éªŒè¯è¾“å…¥
      this.validateStandardRequest(input);

      // è½¬æ¢ä¸ºvLLMç‰¹å®šæ ¼å¼
      const vllmRequest = this.adaptRequest(input);

      // è°ƒç”¨vLLM API
      const vllmResponse = await this.callVLLM(vllmRequest);

      // è½¬æ¢å“åº”ä¸ºæ ‡å‡†æ ¼å¼
      const standardResponse = this.adaptResponse(vllmResponse);

      const processingTime = Date.now() - startTime;
      console.log(`âœ… vLLMå…¼å®¹å¤„ç†å®Œæˆ (${processingTime}ms)`);

      return standardResponse;
    } catch (error) {
      const processingTime = Date.now() - startTime;
      console.error(`âŒ vLLMå…¼å®¹å¤„ç†å¤±è´¥ (${processingTime}ms):`, error.message);
      throw error;
    }
  }

  /**
   * å¥åº·æ£€æŸ¥
   */
  protected async doHealthCheck(): Promise<any> {
    try {
      await this.testConnection();
      return {
        status: 'healthy',
        endpoint: this.config.baseUrl,
        models: this.config.models,
        features: this.config.features,
        vllmConfig: this.config.vllmSpecific,
        timestamp: new Date().toISOString(),
      };
    } catch (error) {
      throw new Error(`vLLMå¥åº·æ£€æŸ¥å¤±è´¥: ${error.message}`);
    }
  }

  /**
   * éªŒè¯æ ‡å‡†è¯·æ±‚
   */
  private validateStandardRequest(request: StandardRequest): void {
    if (!request.model) {
      throw new Error(PIPELINE_ERROR_MESSAGES.VALIDATION.MISSING_PARAMETER('model'));
    }

    if (!request.messages || !Array.isArray(request.messages) || request.messages.length === 0) {
      throw new Error(PIPELINE_ERROR_MESSAGES.VALIDATION.INVALID_MESSAGES);
    }

    // æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒï¼ˆå¦‚æœé…ç½®äº†å…·ä½“æ¨¡å‹ï¼‰
    if (this.config.models.length > 0 && !this.config.models.includes(request.model)) {
      throw new Error(PIPELINE_ERROR_MESSAGES.VALIDATION.MODEL_NOT_SUPPORTED(request.model, this.config.models.join(', ')));
    }
  }

  /**
   * æ ‡å‡†è¯·æ±‚ â†’ vLLMç‰¹å®šæ ¼å¼
   */
  private adaptRequest(standardRequest: StandardRequest): VLLMRequest {
    const vllmRequest: VLLMRequest = {
      ...standardRequest,
    };

    // vLLMç‰¹å®šå‚æ•°æ˜ å°„
    if (standardRequest.frequency_penalty !== undefined) {
      // vLLMä½¿ç”¨repetition_penaltyè€Œä¸æ˜¯frequency_penalty
      vllmRequest.repetition_penalty = 1.0 + standardRequest.frequency_penalty;
    }

    // vLLMçš„temperatureèŒƒå›´é€šå¸¸æ˜¯0-2
    if (standardRequest.temperature !== undefined) {
      vllmRequest.temperature = Math.max(0.001, Math.min(2.0, standardRequest.temperature));
    }

    // vLLMæ”¯æŒæ›´ç²¾ç»†çš„æ§åˆ¶
    if (standardRequest.top_p !== undefined) {
      vllmRequest.top_p = standardRequest.top_p;
    }

    // vLLMç‰¹å®šçš„é«˜æ€§èƒ½å‚æ•°
    if (this.config.features.parallel) {
      vllmRequest.use_beam_search = false; // ç¦ç”¨beam searchä»¥æé«˜å¹¶è¡Œæ€§èƒ½
    }

    return vllmRequest;
  }

  /**
   * è°ƒç”¨vLLM API
   */
  private async callVLLM(request: VLLMRequest): Promise<any> {
    try {
      if (request.stream) {
        throw new Error(PIPELINE_ERROR_MESSAGES.PROCESSING.STREAMING_NOT_IMPLEMENTED);
      }

      // vLLMæ‰©å±•å‚æ•°
      const vllmParams: any = {
        model: request.model,
        messages: request.messages,
        max_tokens: request.max_tokens,
        temperature: request.temperature,
        top_p: request.top_p,
        stop: request.stop,
        stream: false,
      };

      // æ·»åŠ vLLMç‰¹å®šå‚æ•°
      if (request.top_k !== undefined) {
        vllmParams.top_k = request.top_k;
      }
      if (request.repetition_penalty !== undefined) {
        vllmParams.repetition_penalty = request.repetition_penalty;
      }
      if (request.length_penalty !== undefined) {
        vllmParams.length_penalty = request.length_penalty;
      }
      if (request.logprobs !== undefined) {
        vllmParams.logprobs = request.logprobs;
      }

      const response = await this.openaiClient.chat.completions.create(vllmParams);

      return response;
    } catch (error) {
      if (error.name === 'APIError') {
        throw new Error(`vLLM APIé”™è¯¯: ${error.message}`);
      } else if (error.name === 'APIConnectionError') {
        throw new Error(`vLLMè¿æ¥é”™è¯¯: ${error.message}`);
      } else if (error.name === 'APITimeoutError') {
        throw new Error(`vLLMè¯·æ±‚è¶…æ—¶: ${error.message}`);
      } else {
        throw new Error(`vLLMæœªçŸ¥é”™è¯¯: ${error.message}`);
      }
    }
  }

  /**
   * vLLMå“åº” â†’ æ ‡å‡†æ ¼å¼
   */
  private adaptResponse(vllmResponse: any): StandardResponse {
    const standardResponse: StandardResponse = {
      id: vllmResponse.id,
      object: vllmResponse.object,
      created: vllmResponse.created,
      model: vllmResponse.model,
      choices: vllmResponse.choices.map((choice: any) => ({
        index: choice.index,
        message: {
          role: choice.message.role,
          content: choice.message.content,
        },
        finish_reason: choice.finish_reason,
        logprobs: choice.logprobs, // vLLMå¯èƒ½è¿”å›logprobs
      })),
      usage: {
        prompt_tokens: vllmResponse.usage?.prompt_tokens || 0,
        completion_tokens: vllmResponse.usage?.completion_tokens || 0,
        total_tokens: vllmResponse.usage?.total_tokens || 0,
      },
    };

    return standardResponse;
  }

  /**
   * æµ‹è¯•è¿æ¥
   */
  private async testConnection(): Promise<void> {
    try {
      const models = await this.openaiClient.models.list();
      console.log(`ğŸ” æ£€æµ‹åˆ° ${models.data.length} ä¸ªvLLMæ¨¡å‹`);

      if (models.data.length === 0) {
        console.warn('âš ï¸ vLLMæ²¡æœ‰è¿”å›æ¨¡å‹åˆ—è¡¨ï¼Œä½†è¿æ¥æ­£å¸¸');
      }
    } catch (error) {
      throw new Error(`vLLMè¿æ¥æµ‹è¯•å¤±è´¥: ${error.message}`);
    }
  }

  /**
   * è·å–æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
   */
  async getSupportedModels(): Promise<string[]> {
    try {
      const models = await this.openaiClient.models.list();
      return models.data.map(model => model.id);
    } catch (error) {
      console.warn('è·å–vLLMæ¨¡å‹åˆ—è¡¨å¤±è´¥ï¼Œä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹åˆ—è¡¨');
      return this.config.models;
    }
  }

  /**
   * è·å–vLLMç‰¹å®šä¿¡æ¯
   */
  async getVLLMInfo(): Promise<any> {
    try {
      // vLLMå¯èƒ½æä¾›ç‰¹å®šçš„ä¿¡æ¯ç«¯ç‚¹
      // è¿™é‡Œå¯ä»¥æ‰©å±•è·å–GPUä½¿ç”¨ç‡ã€æ¨¡å‹çŠ¶æ€ç­‰ä¿¡æ¯
      return {
        config: this.config.vllmSpecific,
        models: await this.getSupportedModels(),
        features: this.config.features,
      };
    } catch (error) {
      console.warn('è·å–vLLMä¿¡æ¯å¤±è´¥:', error.message);
      return null;
    }
  }
}
