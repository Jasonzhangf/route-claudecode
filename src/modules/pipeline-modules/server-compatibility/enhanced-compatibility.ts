/**
 * Enhanced Server-Compatibility Module - ç²¾ç®€ç‰ˆè®¾è®¡å®ç°
 *
 * è®¾è®¡ç†å¿µï¼š
 * 1. å“åº”åå¤„ç† - ä¿®å¤å„Providerå“åº”æ ¼å¼ä¸ä¸€è‡´é—®é¢˜
 * 2. å‚æ•°èŒƒå›´é€‚é… - è°ƒæ•´è¶…å‡ºProvideré™åˆ¶çš„å‚æ•°
 * 3. é”™è¯¯æ ‡å‡†åŒ– - ç»Ÿä¸€ä¸åŒProviderçš„é”™è¯¯å“åº”æ ¼å¼
 * 4. ç§»é™¤æ¨¡å‹æ˜ å°„ - è¯¥åŠŸèƒ½å±äºRouterå±‚èŒè´£
 *
 * @author Jason Zhang
 */

import { ModuleInterface, ModuleStatus, ModuleType, ModuleMetrics } from '../../../interfaces/module/base-module';
import { EventEmitter } from 'events';
import {
  LMStudioResponseFixer,
  DeepSeekResponseFixer,
  OllamaResponseFixer,
  GenericResponseFixer,
} from './response-compatibility-fixer';

/**
 * æ ‡å‡†OpenAIè¯·æ±‚æ¥å£
 */
export interface OpenAIStandardRequest {
  model: string;
  messages: Array<{
    role: 'system' | 'user' | 'assistant' | 'tool';
    content: string;
    name?: string;
    tool_call_id?: string;
  }>;
  max_tokens?: number;
  temperature?: number;
  top_p?: number;
  frequency_penalty?: number;
  presence_penalty?: number;
  stop?: string | string[];
  stream?: boolean;
  tools?: Array<{
    type: 'function';
    function: {
      name: string;
      description: string;
      parameters: any;
    };
  }>;
  tool_choice?: 'none' | 'auto' | 'required' | { type: 'function'; function: { name: string } };
}

/**
 * æ ‡å‡†OpenAIå“åº”æ¥å£
 */
export interface OpenAIStandardResponse {
  id: string;
  object: 'chat.completion';
  created: number;
  model: string;
  choices: Array<{
    index: number;
    message: {
      role: 'assistant';
      content?: string;
      tool_calls?: Array<{
        id: string;
        type: 'function';
        function: {
          name: string;
          arguments: string;
        };
      }>;
    };
    finish_reason: 'stop' | 'length' | 'tool_calls' | 'content_filter';
  }>;
  usage: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
  system_fingerprint?: string;
  thinking?: string; // DeepSeekç­‰æ¨¡å‹çš„thinkingæ¨¡å¼å­—æ®µ
}

/**
 * æ ‡å‡†OpenAIé”™è¯¯å“åº”æ¥å£
 */
export interface OpenAIErrorResponse {
  error: {
    message: string;
    type: 'api_error' | 'invalid_request_error' | 'authentication_error' | 'rate_limit_error';
    code: string | null;
    param: string | null;
  };
}

/**
 * Providerèƒ½åŠ›é…ç½®æ¥å£
 */
export interface ProviderCapabilities {
  name: string;
  supportsTools: boolean;
  supportsThinking: boolean;
  parameterLimits: {
    temperature?: { min: number; max: number };
    top_p?: { min: number; max: number };
    max_tokens?: { min: number; max: number };
  };
  responseFixesNeeded: string[];
}

/**
 * Debugè®°å½•å™¨æ¥å£ï¼ˆæ¨¡æ‹Ÿï¼‰
 */
export interface DebugRecorder {
  record(eventType: string, data: any): void;
  recordInput(moduleType: string, requestId: string, data: any): void;
  recordOutput(moduleType: string, requestId: string, data: any): void;
  recordError(moduleType: string, requestId: string, data: any): void;
}

/**
 * å“åº”åˆ†æç»“æœæ¥å£
 */
export interface ResponseAnalysis {
  has_id: boolean;
  has_object: boolean;
  has_created: boolean;
  has_choices: boolean;
  choices_count: number;
  has_usage: boolean;
  usage_complete: boolean;
  has_tool_calls: boolean;
  extra_fields: string[];
}

/**
 * Enhanced Server Compatibility Module
 * ä¸“æ³¨äºå“åº”å…¼å®¹æ€§ä¿®å¤ï¼Œä¸å¤„ç†æ¨¡å‹æ˜ å°„
 */
export class EnhancedServerCompatibilityModule extends EventEmitter implements ModuleInterface {
  private readonly id: string = 'enhanced-server-compatibility-module';
  private readonly name: string = 'Enhanced Server Compatibility Module';
  private readonly type: any = 'server-compatibility';
  private readonly version: string = '1.0.0';
  private status: ModuleStatus['health'] = 'healthy';
  private debugRecorder: DebugRecorder;

  // Response fixers
  private lmstudioFixer: LMStudioResponseFixer;
  private deepseekFixer: DeepSeekResponseFixer;
  private ollamaFixer: OllamaResponseFixer;
  private genericFixer: GenericResponseFixer;

  constructor(debugRecorder?: DebugRecorder) {
    super();
    // é›¶Mockupç­–ç•¥ï¼šå¿…é¡»æä¾›çœŸå®çš„debugRecorder
    if (!debugRecorder) {
      throw new Error('Zero Mockup Policy: debugRecorder is required and cannot be mocked');
    }
    this.debugRecorder = debugRecorder;

    // åˆå§‹åŒ–å“åº”ä¿®å¤å™¨
    this.lmstudioFixer = new LMStudioResponseFixer(this.debugRecorder);
    this.deepseekFixer = new DeepSeekResponseFixer(this.debugRecorder);
    this.ollamaFixer = new OllamaResponseFixer(this.debugRecorder);
    this.genericFixer = new GenericResponseFixer(this.debugRecorder);

    console.log('ğŸ”§ åˆå§‹åŒ–å¢å¼ºæœåŠ¡å™¨å…¼å®¹æ€§æ¨¡å—');
  }

  // ModuleInterface å®ç°
  getId(): string {
    return this.id;
  }

  getName(): string {
    return this.name;
  }

  getType(): ModuleType {
    return ModuleType.SERVER_COMPATIBILITY;
  }

  getVersion(): string {
    return this.version;
  }

  getStatus(): ModuleStatus {
    return {
      id: this.id,
      name: this.name,
      type: ModuleType.SERVER_COMPATIBILITY,
      status: 'running',
      health: this.status,
    };
  }

  getMetrics(): ModuleMetrics {
    return {
      requestsProcessed: 0,
      averageProcessingTime: 0,
      errorRate: 0,
      memoryUsage: 0,
      cpuUsage: 0,
    };
  }

  async configure(config: any): Promise<void> {
    // é…ç½®é€»è¾‘
    console.log('ğŸ”§ é…ç½®æœåŠ¡å™¨å…¼å®¹æ€§æ¨¡å—:', config);
  }

  async start(): Promise<void> {
    console.log('â–¶ï¸ å¯åŠ¨æœåŠ¡å™¨å…¼å®¹æ€§æ¨¡å—');
  }

  async stop(): Promise<void> {
    console.log('â¹ï¸ åœæ­¢æœåŠ¡å™¨å…¼å®¹æ€§æ¨¡å—');
  }

  async reset(): Promise<void> {
    // é‡ç½®é€»è¾‘
  }

  async cleanup(): Promise<void> {
    // æ¸…ç†é€»è¾‘
  }

  async healthCheck(): Promise<{ healthy: boolean; details: any }> {
    return { healthy: true, details: { status: 'running' } };
  }

  /**
   * ä¸»å¤„ç†æ–¹æ³• - æ ¹æ®è¾“å…¥ç±»å‹å†³å®šå¤„ç†æ–¹å¼
   */
  async process(input: OpenAIStandardRequest | any): Promise<OpenAIStandardRequest | OpenAIStandardResponse> {
    const startTime = Date.now();

    try {
      if (this.isRequest(input)) {
        console.log('ğŸ”„ [ServerCompatibility] å¤„ç†è¯·æ±‚é€‚é…');
        const serverType = this.detectServerType(input);
        const result = await this.adaptRequest(input as OpenAIStandardRequest, serverType);

        const processingTime = Date.now() - startTime;
        console.log(`âœ… è¯·æ±‚é€‚é…å®Œæˆ (${processingTime}ms)`);
        return result;
      } else {
        console.log('ğŸ”„ [ServerCompatibility] å¤„ç†å“åº”ä¿®å¤');
        const serverType = this.detectServerTypeFromResponse(input);
        const result = await this.adaptResponse(input, serverType);

        const processingTime = Date.now() - startTime;
        console.log(`âœ… å“åº”ä¿®å¤å®Œæˆ (${processingTime}ms)`);
        return result;
      }
    } catch (error) {
      const processingTime = Date.now() - startTime;
      console.error(`âŒ æœåŠ¡å™¨å…¼å®¹æ€§å¤„ç†å¤±è´¥ (${processingTime}ms):`, error.message);
      throw error;
    }
  }

  /**
   * è¯·æ±‚å‚æ•°é€‚é…ï¼ˆè½»é‡çº§ï¼Œæ— æ¨¡å‹æ˜ å°„ï¼‰
   */
  async adaptRequest(request: OpenAIStandardRequest, serverType: string): Promise<OpenAIStandardRequest> {
    const requestId = this.generateRequestId();

    // è®°å½•è¯·æ±‚é€‚é…å‰çŠ¶æ€
    this.debugRecorder.recordInput('server-compatibility-request', requestId, {
      server_type: serverType,
      original_request: request,
      needs_adaptation: this.detectRequestAdaptationNeeds(request, serverType),
    });

    try {
      let adaptedRequest = { ...request };

      // æ ¹æ®æœåŠ¡å™¨ç±»å‹è¿›è¡Œå‚æ•°é€‚é…
      switch (serverType) {
        case 'deepseek':
          adaptedRequest = this.adaptForDeepSeek(adaptedRequest);
          break;
        case 'lmstudio':
          adaptedRequest = this.adaptForLMStudio(adaptedRequest);
          break;
        case 'ollama':
          adaptedRequest = this.adaptForOllama(adaptedRequest);
          break;
        default:
          // é€šç”¨é€‚é…ï¼Œä¸»è¦æ˜¯å‚æ•°èŒƒå›´æ£€æŸ¥
          adaptedRequest = this.adaptGeneric(adaptedRequest);
      }

      // è®°å½•é€‚é…åçŠ¶æ€
      this.debugRecorder.recordOutput('server-compatibility-request', requestId, {
        server_type: serverType,
        adapted_request: adaptedRequest,
        adaptations_applied: this.getAppliedAdaptations(request, adaptedRequest),
      });

      return adaptedRequest;
    } catch (error) {
      this.debugRecorder.recordError('server-compatibility-request', requestId, {
        server_type: serverType,
        error_type: error.constructor.name,
        error_message: error.message,
        original_request: request,
      });
      throw error;
    }
  }

  /**
   * å“åº”å…¼å®¹æ€§ä¿®å¤ï¼ˆé‡ç‚¹åŠŸèƒ½ï¼‰
   */
  async adaptResponse(response: any, serverType: string): Promise<OpenAIStandardResponse> {
    const requestId = this.generateRequestId();

    // è®°å½•å“åº”ä¿®å¤å‰çŠ¶æ€
    this.debugRecorder.recordInput('server-compatibility-response', requestId, {
      server_type: serverType,
      original_response: response,
      response_analysis: this.analyzeResponse(response),
      fixes_needed: this.detectNeededFixes(response, serverType),
    });

    try {
      let fixedResponse: OpenAIStandardResponse;

      // æ ¹æ®æœåŠ¡å™¨ç±»å‹é€‰æ‹©ä¿®å¤ç­–ç•¥
      switch (serverType) {
        case 'lmstudio':
          fixedResponse = await this.lmstudioFixer.fixResponse(response);
          break;
        case 'deepseek':
          fixedResponse = await this.deepseekFixer.fixResponse(response);
          break;
        case 'ollama':
          fixedResponse = await this.ollamaFixer.fixResponse(response);
          break;
        default:
          fixedResponse = await this.genericFixer.fixResponse(response);
      }

      // è®°å½•ä¿®å¤åçŠ¶æ€
      this.debugRecorder.recordOutput('server-compatibility-response', requestId, {
        server_type: serverType,
        fixed_response: fixedResponse,
        fixes_applied: this.getAppliedFixes(response, fixedResponse),
        validation_passed: this.validateResponse(fixedResponse),
      });

      return fixedResponse;
    } catch (error) {
      this.debugRecorder.recordError('server-compatibility-response', requestId, {
        server_type: serverType,
        error_type: error.constructor.name,
        error_message: error.message,
        original_response: response,
      });
      throw error;
    }
  }

  /**
   * é”™è¯¯å“åº”æ ‡å‡†åŒ–
   */
  async normalizeError(error: any, serverType: string): Promise<OpenAIErrorResponse> {
    const baseError: OpenAIErrorResponse = {
      error: {
        message: '',
        type: 'api_error',
        code: null,
        param: null,
      },
    };

    this.debugRecorder.record('error_normalization', {
      server_type: serverType,
      original_error_type: error.constructor?.name,
      original_error_message: error.message,
    });

    switch (serverType) {
      case 'lmstudio':
        return this.normalizeLMStudioError(error, baseError);
      case 'deepseek':
        return this.normalizeDeepSeekError(error, baseError);
      case 'ollama':
        return this.normalizeOllamaError(error, baseError);
      default:
        return this.normalizeGenericError(error, baseError);
    }
  }

  /**
   * è·å–Providerèƒ½åŠ›é…ç½®
   */
  getProviderCapabilities(serverType: string): ProviderCapabilities {
    return this.getCapabilitiesForProvider(serverType);
  }

  // ç§æœ‰è¾…åŠ©æ–¹æ³•

  /**
   * åˆ¤æ–­è¾“å…¥æ˜¯å¦ä¸ºè¯·æ±‚æ ¼å¼
   */
  private isRequest(input: any): boolean {
    return (
      input &&
      typeof input.model === 'string' &&
      Array.isArray(input.messages) &&
      input.messages.length > 0 &&
      !input.id && // å“åº”é€šå¸¸æœ‰id
      !input.choices && // å“åº”æœ‰choices
      !input.usage
    ); // å“åº”æœ‰usage
  }

  /**
   * ä»è¯·æ±‚ä¸­æ£€æµ‹æœåŠ¡å™¨ç±»å‹
   */
  private detectServerType(request: OpenAIStandardRequest): string {
    // ç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥ä»é…ç½®æˆ–å…¶ä»–ä¸Šä¸‹æ–‡è·å–
    // è¿™é‡ŒåŸºäºæ¨¡å‹åç§°æˆ–å…¶ä»–ç‰¹å¾æ¨æ–­
    if (request.model.includes('deepseek')) return 'deepseek';
    if (request.model.includes('qwen') || request.model.includes('mlx')) return 'lmstudio';
    if (request.model.includes('ollama')) return 'ollama';
    return 'generic';
  }

  /**
   * ä»å“åº”ä¸­æ£€æµ‹æœåŠ¡å™¨ç±»å‹
   */
  private detectServerTypeFromResponse(response: any): string {
    // åŸºäºå“åº”ç‰¹å¾æ£€æµ‹æœåŠ¡å™¨ç±»å‹
    if (response.thinking) return 'deepseek'; // DeepSeekç‰¹æœ‰
    if (response.model && (response.model.includes('qwen') || response.model.includes('mlx'))) return 'lmstudio';
    if (response.model && response.model.includes('ollama')) return 'ollama';
    return 'generic';
  }

  /**
   * DeepSeekè¯·æ±‚é€‚é…
   */
  private adaptForDeepSeek(request: OpenAIStandardRequest): OpenAIStandardRequest {
    const adapted = { ...request };

    // DeepSeekå·¥å…·è°ƒç”¨ä¼˜åŒ–
    if (adapted.tools && adapted.tools.length > 0) {
      adapted.tool_choice = adapted.tool_choice || 'auto';
    }

    // å‚æ•°èŒƒå›´é™åˆ¶
    if (adapted.max_tokens && adapted.max_tokens > 8192) {
      adapted.max_tokens = 8192;
      this.debugRecorder.record('deepseek_max_tokens_adjusted', {
        original: request.max_tokens,
        adjusted: adapted.max_tokens,
      });
    }

    if (adapted.temperature && (adapted.temperature < 0.01 || adapted.temperature > 2.0)) {
      adapted.temperature = Math.max(0.01, Math.min(2.0, adapted.temperature));
    }

    return adapted;
  }

  /**
   * LM Studioè¯·æ±‚é€‚é… - æ¢å¤æ­£ç¡®é€»è¾‘ï¼šåªåšå‚æ•°è°ƒæ•´ï¼Œä¸è½¬æ¢åè®®
   */
  private adaptForLMStudio(request: OpenAIStandardRequest): OpenAIStandardRequest {
    const adapted = { ...request };

    // LM Studioæ”¯æŒå·¥å…·è°ƒç”¨ï¼Œä½†éœ€è¦ç¡®ä¿æ ¼å¼æ­£ç¡®
    if (adapted.tools && Array.isArray(adapted.tools)) {
      // åªåšæ ¼å¼éªŒè¯å’Œä¿®å¤ï¼Œä¸åšåè®®è½¬æ¢
      adapted.tools = adapted.tools.filter(tool => {
        return tool && tool.type === 'function' && tool.function && tool.function.name;
      });
      
      this.debugRecorder.record('lmstudio_tools_validated', {
        original_count: request.tools?.length || 0,
        validated_count: adapted.tools.length
      });
      
      // ç¡®ä¿tool_choiceæ ¼å¼æ­£ç¡®
      if (adapted.tool_choice && typeof adapted.tool_choice === 'object') {
        // éªŒè¯å¯¹è±¡æ ¼å¼çš„tool_choice
        if (!adapted.tool_choice.type || !adapted.tool_choice.function?.name) {
          adapted.tool_choice = 'auto';
        }
      }
    }

    // å‚æ•°é™åˆ¶è°ƒæ•´
    if (adapted.temperature && adapted.temperature > 2.0) {
      adapted.temperature = 2.0;
    }

    if (adapted.max_tokens && adapted.max_tokens > 4096) {
      adapted.max_tokens = 4096;
    }

    return adapted;
  }

  /**
   * Ollamaè¯·æ±‚é€‚é…
   */
  private adaptForOllama(request: OpenAIStandardRequest): OpenAIStandardRequest {
    const adapted = { ...request };

    // Ollamaå‚æ•°é™åˆ¶
    if (adapted.temperature && (adapted.temperature < 0 || adapted.temperature > 2.0)) {
      adapted.temperature = Math.max(0, Math.min(2.0, adapted.temperature));
    }

    // Ollamaé€šå¸¸ä¸æ”¯æŒå·¥å…·è°ƒç”¨
    if (adapted.tools) {
      delete adapted.tools;
      delete adapted.tool_choice;
    }

    return adapted;
  }

  /**
   * é€šç”¨è¯·æ±‚é€‚é…
   */
  private adaptGeneric(request: OpenAIStandardRequest): OpenAIStandardRequest {
    const adapted = { ...request };

    // é€šç”¨å‚æ•°èŒƒå›´æ£€æŸ¥
    if (adapted.temperature && (adapted.temperature < 0 || adapted.temperature > 2.0)) {
      adapted.temperature = Math.max(0, Math.min(2.0, adapted.temperature));
    }

    if (adapted.top_p && (adapted.top_p < 0 || adapted.top_p > 1.0)) {
      adapted.top_p = Math.max(0, Math.min(1.0, adapted.top_p));
    }

    return adapted;
  }

  /**
   * LM Studioå“åº”ä¿®å¤
   */
  private async fixLMStudioResponse(response: any): Promise<OpenAIStandardResponse> {
    this.debugRecorder.record('lmstudio_response_fix_start', {
      original_structure: this.analyzeResponseStructure(response),
      has_usage: !!response.usage,
      has_choices: !!response.choices,
    });

    // å¿…éœ€å­—æ®µè¡¥å…¨
    const fixedResponse: OpenAIStandardResponse = {
      id: response.id || `chatcmpl-lms-${Date.now()}${Math.random().toString(36).substr(2, 9)}`,
      object: 'chat.completion',
      created: response.created || Math.floor(Date.now() / 1000),
      model: response.model || 'local-model',
      choices: this.fixChoicesArray(response.choices || []),
      usage: this.fixUsageStatistics(response.usage),
      system_fingerprint: response.system_fingerprint,
    };

    // è®°å½•ä¿®å¤æ“ä½œ
    this.debugRecorder.record('lmstudio_response_fix_completed', {
      fixes_applied: this.getAppliedFixes(response, fixedResponse),
      final_structure_valid: this.validateOpenAIFormat(fixedResponse),
    });

    return fixedResponse;
  }

  /**
   * DeepSeekå“åº”ä¿®å¤
   */
  private async fixDeepSeekResponse(response: any): Promise<OpenAIStandardResponse> {
    // DeepSeeké€šå¸¸è¿”å›æ ‡å‡†æ ¼å¼ï¼Œä½†å¤„ç†æ€è€ƒæ¨¡å¼ç‰¹æ®Šæƒ…å†µ
    const fixedResponse: OpenAIStandardResponse = {
      ...response,
      object: 'chat.completion', // ç¡®ä¿objectå­—æ®µæ­£ç¡®
    };

    // å¤„ç†æ€è€ƒæ¨¡å¼çš„ç‰¹æ®Šå“åº”
    if (response.thinking && response.thinking.length > 0) {
      this.debugRecorder.record('deepseek_thinking_mode_detected', {
        thinking_content_length: response.thinking.length,
        has_reasoning_chain: true,
      });
      // æ€è€ƒå†…å®¹ä¸æš´éœ²ç»™å®¢æˆ·ç«¯ï¼Œä»…è®°å½•è°ƒè¯•ä¿¡æ¯
      delete fixedResponse.thinking; // ç§»é™¤éæ ‡å‡†å­—æ®µ
    }

    // å·¥å…·è°ƒç”¨æ ¼å¼ç¡®ä¿æ­£ç¡®
    if (fixedResponse.choices) {
      fixedResponse.choices = fixedResponse.choices.map(choice => {
        if (choice.message?.tool_calls) {
          choice.message.tool_calls = choice.message.tool_calls.map(toolCall => ({
            ...toolCall,
            function: {
              ...toolCall.function,
              arguments:
                typeof toolCall.function?.arguments === 'string'
                  ? toolCall.function.arguments
                  : JSON.stringify(toolCall.function?.arguments || {}),
            },
          }));
        }
        return choice;
      });
    }

    return fixedResponse;
  }

  /**
   * Ollamaå“åº”ä¿®å¤
   */
  private async fixOllamaResponse(response: any): Promise<OpenAIStandardResponse> {
    // Ollamaå“åº”ä¿®å¤é€»è¾‘
    return this.fixGenericResponse(response);
  }

  /**
   * é€šç”¨å“åº”ä¿®å¤
   */
  private async fixGenericResponse(response: any): Promise<OpenAIStandardResponse> {
    const chatId = `chatcmpl-${this.generateUUID()}`;
    const timestamp = Math.floor(Date.now() / 1000);

    // å¦‚æœå·²ç»æ˜¯æ ‡å‡†æ ¼å¼ï¼Œè¿›è¡ŒåŸºç¡€ä¿®å¤
    if (this.isStandardOpenAIResponse(response)) {
      return {
        ...response,
        id: response.id || chatId,
        created: response.created || timestamp,
        usage: this.fixUsageStatistics(response.usage),
      };
    }

    // ä»å„ç§å¯èƒ½çš„å“åº”æ ¼å¼ä¸­æå–å†…å®¹
    let content = '';
    if (typeof response === 'string') {
      content = response;
    } else if (response.content) {
      content = typeof response.content === 'string' ? response.content : JSON.stringify(response.content);
    } else if (response.choices?.[0]?.message?.content) {
      content = response.choices[0].message.content;
    } else if (response.message) {
      content = typeof response.message === 'string' ? response.message : JSON.stringify(response.message);
    }

    const fixedResponse: OpenAIStandardResponse = {
      id: chatId,
      object: 'chat.completion',
      created: timestamp,
      model: response.model || 'generic-model',
      choices: [
        {
          index: 0,
          message: {
            role: 'assistant',
            content: content,
          },
          finish_reason: 'stop',
        },
      ],
      usage: {
        prompt_tokens: response.usage?.prompt_tokens || 0,
        completion_tokens: response.usage?.completion_tokens || 0,
        total_tokens: response.usage?.total_tokens || 0,
      },
    };

    return fixedResponse;
  }

  /**
   * ä¿®å¤ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯
   */
  private fixUsageStatistics(usage: any): { prompt_tokens: number; completion_tokens: number; total_tokens: number } {
    const fixedUsage = {
      prompt_tokens: usage?.prompt_tokens || 0,
      completion_tokens: usage?.completion_tokens || 0,
      total_tokens: usage?.total_tokens || 0,
    };

    // è‡ªåŠ¨è®¡ç®—total_tokenså¦‚æœç¼ºå¤±
    if (fixedUsage.total_tokens === 0) {
      fixedUsage.total_tokens = fixedUsage.prompt_tokens + fixedUsage.completion_tokens;
    }

    return fixedUsage;
  }

  /**
   * ä¿®å¤choicesæ•°ç»„
   */
  private fixChoicesArray(choices: any[]): OpenAIStandardResponse['choices'] {
    if (!Array.isArray(choices) || choices.length === 0) {
      return [
        {
          index: 0,
          message: { role: 'assistant', content: '' },
          finish_reason: 'stop',
        },
      ];
    }

    return choices.map((choice, index) => ({
      index: choice.index ?? index,
      message: {
        role: 'assistant',
        content: choice.message?.content || '',
        tool_calls: choice.message?.tool_calls ? this.fixToolCallsFormat(choice.message.tool_calls) : undefined,
      },
      finish_reason: choice.finish_reason || 'stop',
    }));
  }

  /**
   * ä¿®å¤å·¥å…·è°ƒç”¨æ ¼å¼
   */
  private fixToolCallsFormat(
    toolCalls: any[]
  ): Array<{ id: string; type: 'function'; function: { name: string; arguments: string } }> {
    return toolCalls.map(toolCall => ({
      id: toolCall.id || `call_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
      type: 'function',
      function: {
        name: toolCall.function?.name || '',
        arguments:
          typeof toolCall.function?.arguments === 'string'
            ? toolCall.function.arguments
            : JSON.stringify(toolCall.function?.arguments || {}),
      },
    }));
  }

  /**
   * LM Studioé”™è¯¯æ ‡å‡†åŒ–
   */
  private normalizeLMStudioError(error: any, baseError: OpenAIErrorResponse): OpenAIErrorResponse {
    if (error.message?.includes('model not loaded')) {
      baseError.error.message = 'Model not available on local server';
      baseError.error.type = 'invalid_request_error';
      baseError.error.code = 'model_not_found';
    } else if (error.message?.includes('context length')) {
      baseError.error.message = 'Request exceeds maximum context length';
      baseError.error.type = 'invalid_request_error';
      baseError.error.code = 'context_length_exceeded';
    } else {
      baseError.error.message = error.message || 'LM Studio server error';
      baseError.error.type = 'api_error';
    }

    return baseError;
  }

  /**
   * DeepSeeké”™è¯¯æ ‡å‡†åŒ–
   */
  private normalizeDeepSeekError(error: any, baseError: OpenAIErrorResponse): OpenAIErrorResponse {
    if (error.status === 429) {
      baseError.error.message = 'Rate limit exceeded';
      baseError.error.type = 'rate_limit_error';
      baseError.error.code = 'rate_limit_exceeded';
    } else if (error.status === 401) {
      baseError.error.message = 'Invalid API key';
      baseError.error.type = 'authentication_error';
      baseError.error.code = 'invalid_api_key';
    } else {
      baseError.error.message = error.message || 'DeepSeek API error';
      baseError.error.type = 'api_error';
    }

    return baseError;
  }

  /**
   * Ollamaé”™è¯¯æ ‡å‡†åŒ–
   */
  private normalizeOllamaError(error: any, baseError: OpenAIErrorResponse): OpenAIErrorResponse {
    baseError.error.message = error.message || 'Ollama server error';
    baseError.error.type = 'api_error';
    return baseError;
  }

  /**
   * é€šç”¨é”™è¯¯æ ‡å‡†åŒ–
   */
  private normalizeGenericError(error: any, baseError: OpenAIErrorResponse): OpenAIErrorResponse {
    baseError.error.message = error.message || 'Unknown server error';
    baseError.error.type = 'api_error';
    return baseError;
  }

  /**
   * Providerèƒ½åŠ›é…ç½®è·å–
   */
  private getCapabilitiesForProvider(serverType: string): ProviderCapabilities {
    const capabilities: Record<string, ProviderCapabilities> = {
      deepseek: {
        name: 'deepseek',
        supportsTools: true,
        supportsThinking: true,
        parameterLimits: {
          temperature: { min: 0.01, max: 2.0 },
          top_p: { min: 0.01, max: 1.0 },
          max_tokens: { min: 1, max: 8192 },
        },
        responseFixesNeeded: ['tool_calls_format', 'thinking_mode_cleanup'],
      },

      lmstudio: {
        name: 'lmstudio',
        supportsTools: false,
        supportsThinking: false,
        parameterLimits: {
          temperature: { min: 0.01, max: 2.0 },
          top_p: { min: 0.01, max: 1.0 },
          max_tokens: { min: 1, max: 4096 },
        },
        responseFixesNeeded: ['missing_usage', 'missing_id', 'missing_created', 'choices_array_fix'],
      },

      ollama: {
        name: 'ollama',
        supportsTools: false,
        supportsThinking: false,
        parameterLimits: {
          temperature: { min: 0, max: 2.0 },
          top_p: { min: 0, max: 1.0 },
          max_tokens: { min: 1, max: 8192 },
        },
        responseFixesNeeded: ['format_standardization', 'usage_calculation'],
      },
    };

    return (
      capabilities[serverType] || {
        name: 'unknown',
        supportsTools: false,
        supportsThinking: false,
        parameterLimits: {
          temperature: { min: 0, max: 2.0 },
          top_p: { min: 0, max: 1.0 },
          max_tokens: { min: 1, max: 4096 },
        },
        responseFixesNeeded: ['basic_standardization'],
      }
    );
  }

  // è¾…åŠ©æ–¹æ³•

  private analyzeResponse(response: any): ResponseAnalysis {
    return {
      has_id: !!response.id,
      has_object: !!response.object,
      has_created: !!response.created,
      has_choices: Array.isArray(response.choices),
      choices_count: Array.isArray(response.choices) ? response.choices.length : 0,
      has_usage: !!response.usage,
      usage_complete: response.usage && response.usage.total_tokens > 0,
      has_tool_calls: response.choices?.[0]?.message?.tool_calls?.length > 0,
      extra_fields: Object.keys(response).filter(
        key => !['id', 'object', 'created', 'model', 'choices', 'usage', 'system_fingerprint'].includes(key)
      ),
    };
  }

  private analyzeResponseStructure(response: any): any {
    return {
      type: typeof response,
      keys: Object.keys(response || {}),
      has_standard_fields: {
        id: !!response?.id,
        object: !!response?.object,
        choices: Array.isArray(response?.choices),
        usage: !!response?.usage,
      },
    };
  }

  private detectRequestAdaptationNeeds(request: OpenAIStandardRequest, serverType: string): string[] {
    const needs = [];
    const capabilities = this.getCapabilitiesForProvider(serverType);

    if (request.tools && !capabilities.supportsTools) {
      needs.push('remove_tools');
    }

    if (
      request.temperature &&
      (request.temperature < capabilities.parameterLimits.temperature?.min ||
        request.temperature > capabilities.parameterLimits.temperature?.max)
    ) {
      needs.push('adjust_temperature');
    }

    return needs;
  }

  private detectNeededFixes(response: any, serverType: string): string[] {
    const capabilities = this.getCapabilitiesForProvider(serverType);
    return capabilities.responseFixesNeeded;
  }

  private getAppliedAdaptations(original: OpenAIStandardRequest, adapted: OpenAIStandardRequest): string[] {
    const adaptations = [];

    if (original.tools && !adapted.tools) {
      adaptations.push('removed_tools');
    }

    if (original.max_tokens !== adapted.max_tokens) {
      adaptations.push('adjusted_max_tokens');
    }

    if (original.temperature !== adapted.temperature) {
      adaptations.push('adjusted_temperature');
    }

    return adaptations;
  }

  private getAppliedFixes(original: any, fixed: OpenAIStandardResponse): string[] {
    const fixes = [];

    if (!original.id && fixed.id) {
      fixes.push('added_id');
    }

    if (!original.created && fixed.created) {
      fixes.push('added_created');
    }

    if (!original.usage || !original.usage.total_tokens) {
      fixes.push('fixed_usage_statistics');
    }

    return fixes;
  }

  private validateResponse(response: OpenAIStandardResponse): boolean {
    return !!(
      response.id &&
      response.object === 'chat.completion' &&
      response.choices &&
      Array.isArray(response.choices) &&
      response.usage &&
      typeof response.usage.total_tokens === 'number'
    );
  }

  private validateOpenAIFormat(response: any): boolean {
    return this.validateResponse(response);
  }

  private isStandardOpenAIResponse(response: any): boolean {
    return (
      response &&
      response.object === 'chat.completion' &&
      Array.isArray(response.choices) &&
      response.usage &&
      typeof response.usage.total_tokens === 'number'
    );
  }

  private generateRequestId(): string {
    return `req_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
  }

  private generateUUID(): string {
    return 'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx'.replace(/[xy]/g, function (c) {
      const r = (Math.random() * 16) | 0;
      const v = c == 'x' ? r : (r & 0x3) | 0x8;
      return v.toString(16);
    });
  }

  /**
   * REMOVED: createMockDebugRecorder violates Zero Mockup Policy
   */
  private createMockDebugRecorder(): never {
    throw new Error('Zero Mockup Policy: Mock debug recorder is not allowed');
  }

  /**
   * æ¸…ç†Debugæ•°æ®ï¼Œç§»é™¤æ•æ„Ÿä¿¡æ¯
   */
  private sanitizeDebugData(data: any): any {
    if (!data || typeof data !== 'object') {
      return data;
    }

    const sanitized = { ...data };

    // ç§»é™¤å¯èƒ½çš„æ•æ„Ÿä¿¡æ¯
    const sensitiveFields = ['api_key', 'authorization', 'token', 'password', 'secret'];

    const recursiveSanitize = (obj: any): any => {
      if (!obj || typeof obj !== 'object') {
        return obj;
      }

      if (Array.isArray(obj)) {
        return obj.map(recursiveSanitize);
      }

      const result: any = {};
      for (const [key, value] of Object.entries(obj)) {
        const lowerKey = key.toLowerCase();

        if (sensitiveFields.some(field => lowerKey.includes(field))) {
          result[key] = '***REDACTED***';
        } else if (typeof value === 'object') {
          result[key] = recursiveSanitize(value);
        } else {
          result[key] = value;
        }
      }

      return result;
    };

    return recursiveSanitize(sanitized);
  }

  /**
   * å†™å…¥Debugæ—¥å¿—åˆ°ç³»ç»Ÿ
   */
  private writeDebugLogToSystem(eventType: string, data: any): void {
    // è¿™é‡Œå¯ä»¥é›†æˆå®é™…çš„æ—¥å¿—ç³»ç»Ÿ
    // ä¾‹å¦‚ï¼šå†™å…¥æ–‡ä»¶ã€å‘é€åˆ°å¤–éƒ¨æœåŠ¡ã€å­˜å‚¨åˆ°æ•°æ®åº“ç­‰

    const debugEntry = {
      timestamp: Date.now(),
      eventType,
      module: 'server-compatibility',
      data,
      environment: {
        nodeVersion: process.version,
        platform: process.platform,
        memoryUsage: process.memoryUsage(),
      },
    };

    // æ¨¡æ‹Ÿå†™å…¥æ“ä½œ - åœ¨å®é™…å®ç°ä¸­å¯ä»¥æ›¿æ¢ä¸ºçœŸå®çš„å­˜å‚¨é€»è¾‘
    if (process.env.DEBUG_LOG_FILE) {
      // å†™å…¥æ–‡ä»¶çš„é€»è¾‘
      console.log(`ğŸ“ [DebugLog] Would write to ${process.env.DEBUG_LOG_FILE}:`, debugEntry);
    }

    if (process.env.DEBUG_WEBHOOK_URL) {
      // å‘é€åˆ°webhookçš„é€»è¾‘
      console.log(`ğŸ“¡ [DebugWebhook] Would send to ${process.env.DEBUG_WEBHOOK_URL}:`, debugEntry);
    }
  }

  /**
   * ç¡®å®šé”™è¯¯ä¸¥é‡ç¨‹åº¦
   */
  private determineErrorSeverity(data: any): 'low' | 'medium' | 'high' | 'critical' {
    if (data?.error_type === 'ValidationError' || data?.error_message?.includes('parameter')) {
      return 'low';
    }

    if (data?.error_type === 'NetworkError' || data?.error_message?.includes('timeout')) {
      return 'medium';
    }

    if (data?.error_type === 'AuthenticationError' || data?.error_message?.includes('unauthorized')) {
      return 'high';
    }

    if (data?.error_type === 'SystemError' || data?.error_message?.includes('critical')) {
      return 'critical';
    }

    return 'medium'; // é»˜è®¤ä¸­ç­‰ä¸¥é‡ç¨‹åº¦
  }

}
