/**
 * V3.0 Response Pipeline - Transformer Layer
 * è´Ÿè´£OpenAIæ ¼å¼ â†’ Anthropicæ ¼å¼çš„è½¬æ¢
 */
export class ResponsePipeline {
    constructor() {
        console.log('ğŸ”§ V3 ResponsePipeline initialized');
    }
    
    async process(response, context) {
        console.log(`ğŸ”„ Processing response for ${context?.provider} [${context?.requestId}]`);
        
        // æ ¹æ®providerç±»å‹è¿›è¡Œæ ¼å¼è½¬æ¢
        const providerType = this.getProviderType(context);
        
        if (providerType === 'openai' || providerType === 'lmstudio') {
            return this.transformOpenAIToAnthropic(response, context);
        }
        
        // å…¶ä»–providerç±»å‹çš„è½¬æ¢
        return response;
    }
    
    /**
     * OpenAIæ ¼å¼ â†’ Anthropicæ ¼å¼è½¬æ¢ (çœŸæ­£çš„Transformer)
     */
    transformOpenAIToAnthropic(response, context) {
        if (!response || !response.choices || !Array.isArray(response.choices)) {
            return response;
        }
        
        const choice = response.choices[0];
        if (!choice || !choice.message) {
            return response;
        }
        
        const message = choice.message;
        const anthropicResponse = {
            id: response.id || `msg-${Date.now()}`,
            type: 'message',
            role: 'assistant',
            content: this.convertContent(message),
            model: response.model,
            stop_reason: this.mapStopReason(choice.finish_reason, message),
            usage: {
                input_tokens: response.usage?.prompt_tokens || 0,
                output_tokens: response.usage?.completion_tokens || 0,
                total_tokens: response.usage?.total_tokens || 0
            }
        };
        
        return anthropicResponse;
    }
    
    /**
     * è½¬æ¢æ¶ˆæ¯å†…å®¹ï¼šOpenAI tool_calls â†’ Anthropic tool_use
     */
    convertContent(message) {
        const content = [];
        
        // æ·»åŠ æ–‡æœ¬å†…å®¹
        if (message.content) {
            content.push({
                type: 'text',
                text: message.content
            });
        }
        
        // è½¬æ¢å·¥å…·è°ƒç”¨ï¼šOpenAI tool_calls â†’ Anthropic tool_use
        if (message.tool_calls && Array.isArray(message.tool_calls)) {
            message.tool_calls.forEach(toolCall => {
                content.push({
                    type: 'tool_use',
                    id: toolCall.id,
                    name: toolCall.function.name,
                    input: JSON.parse(toolCall.function.arguments || '{}')
                });
            });
        }
        
        return content.length > 0 ? content : [{
            type: 'text', 
            text: ''
        }];
    }
    
    /**
     * æ˜ å°„åœæ­¢åŸå› ï¼šOpenAI finish_reason â†’ Anthropic stop_reason
     */
    mapStopReason(finishReason, message) {
        // é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
        if (message.tool_calls && Array.isArray(message.tool_calls) && message.tool_calls.length > 0) {
            return 'tool_use';
        }
        
        // æ ¹æ®finish_reasonæ˜ å°„
        switch (finishReason) {
            case 'tool_calls':
                return 'tool_use';
            case 'stop':
                return 'end_turn';
            case 'length':
                return 'max_tokens';
            default:
                return 'end_turn';
        }
    }
    
    /**
     * è·å–providerç±»å‹
     */
    getProviderType(context) {
        // æ ¹æ®contextåˆ¤æ–­providerç±»å‹
        if (context?.provider === 'lmstudio') {
            return 'openai'; // LM Studioä½¿ç”¨OpenAIåè®®
        }
        
        return context?.providerType || 'unknown';
    }
}
