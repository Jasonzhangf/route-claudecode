/**
 * ç»Ÿä¸€Preprocessingè¡¥ä¸ç³»ç»Ÿ
 * å°†åŸæœ¬åˆ†æ•£çš„è¡¥ä¸æ£€æµ‹å’Œåº”ç”¨ç»Ÿä¸€åˆ°Preprocessingé˜¶æ®µ
 * ç¡®ä¿æ‰€æœ‰è¾“å…¥éƒ½ç»è¿‡ç»Ÿä¸€çš„è¡¥ä¸æ£€æµ‹å’Œå¤„ç†ï¼Œé¿å…é—æ¼
 */

import { PatchManager, createPatchManager } from '../patches';
import { getLogger } from '../logging';
import { 
  BasePatch, 
  PatchContext, 
  PatchResult, 
  Provider 
} from '../patches/types';

interface PreprocessingContext {
  requestId: string;
  provider: Provider;
  model: string;
  stage: 'input' | 'response' | 'streaming';
  timestamp: number;
  metadata?: any;
}

export interface UnifiedPatchPreprocessorConfig {
  enabled: boolean;
  debugMode: boolean;
  forceAllInputs: boolean; // å¼ºåˆ¶æ‰€æœ‰è¾“å…¥éƒ½è¿›å…¥Preprocessing
  bypassConditions: string[]; // å¯ä»¥ç»•è¿‡çš„ç‰¹æ®Šæ¡ä»¶
  performanceTracking: boolean;
  cacheResults: boolean;
  validateFinishReason: boolean; // ğŸ†• æ§åˆ¶æ˜¯å¦éªŒè¯finish reason
  strictFinishReasonValidation: boolean; // ğŸ†• ä¸¥æ ¼æ¨¡å¼ï¼šé‡åˆ°unknownå°±æŠ¥é”™
}

export class UnifiedPatchPreprocessor {
  private patchManager: PatchManager;
  private logger: ReturnType<typeof getLogger>;
  private config: UnifiedPatchPreprocessorConfig;
  private processedCache: Map<string, any> = new Map();
  private performanceMetrics = {
    totalProcessed: 0,
    totalDuration: 0,
    byStage: {
      input: { count: 0, duration: 0 },
      response: { count: 0, duration: 0 },
      streaming: { count: 0, duration: 0 }
    }
  };

  constructor(port?: number, config?: Partial<UnifiedPatchPreprocessorConfig>) {
    this.config = {
      enabled: process.env.RCC_UNIFIED_PREPROCESSING !== 'false',
      debugMode: process.env.RCC_PREPROCESSING_DEBUG === 'true',
      forceAllInputs: process.env.RCC_FORCE_ALL_INPUTS === 'true',
      bypassConditions: [],
      performanceTracking: true,
      cacheResults: process.env.RCC_CACHE_PREPROCESSING === 'true',
      // ğŸ¯ å¼ºåŒ–Tool callæ£€æµ‹ - ä¸å¯é…ç½®å…³é—­ï¼Œå¼ºåˆ¶å¯ç”¨
      validateFinishReason: true, // å¼ºåˆ¶å¯ç”¨ï¼Œå¿½ç•¥ç¯å¢ƒå˜é‡
      strictFinishReasonValidation: process.env.RCC_STRICT_FINISH_REASON === 'true', // é»˜è®¤å…³é—­
      ...config
    };

    // ğŸš¨ å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿å…³é”®éªŒè¯ä¸è¢«é…ç½®è¦†ç›–
    this.config.validateFinishReason = true; // å¼ºåˆ¶é‡ç½®ä¸ºtrue

    this.logger = getLogger(port);
    this.patchManager = createPatchManager(port);

    if (this.config.debugMode) {
      this.logger.info('UnifiedPatchPreprocessor initialized', {
        config: this.config,
        port
      });
    }
  }

  /**
   * ç»Ÿä¸€Preprocessingå…¥å£ï¼šå¤„ç†è¾“å…¥é˜¶æ®µæ•°æ®
   * æ‰€æœ‰APIRequestéƒ½å¿…é¡»ç»è¿‡æ­¤å¤„ç†
   */
  async preprocessInput(
    inputData: any,
    provider: Provider,
    model: string,
    requestId: string
  ): Promise<any> {
    const context: PreprocessingContext = {
      requestId,
      provider,
      model,
      stage: 'input',
      timestamp: Date.now(),
      metadata: inputData.metadata
    };

    return this.processWithUnifiedPipeline(inputData, context);
  }

  /**
   * ç»Ÿä¸€Preprocessingå…¥å£ï¼šå¤„ç†Responseé˜¶æ®µæ•°æ®
   * æ‰€æœ‰ProviderResponseéƒ½å¿…é¡»ç»è¿‡æ­¤å¤„ç†
   */
  async preprocessResponse(
    responseData: any,
    provider: Provider,
    model: string,
    requestId: string
  ): Promise<any> {
    const context: PreprocessingContext = {
      requestId,
      provider,
      model,
      stage: 'response',
      timestamp: Date.now()
    };

    return this.processWithUnifiedPipeline(responseData, context);
  }

  /**
   * ç»Ÿä¸€Preprocessingå…¥å£ï¼šå¤„ç†Streamingæ•°æ®å—
   * æ‰€æœ‰StreamingResponseéƒ½å¿…é¡»ç»è¿‡æ­¤å¤„ç†
   */
  async preprocessStreaming(
    chunkData: any,
    provider: Provider,
    model: string,
    requestId: string
  ): Promise<any> {
    const context: PreprocessingContext = {
      requestId,
      provider,
      model,
      stage: 'streaming',
      timestamp: Date.now()
    };

    return this.processWithUnifiedPipeline(chunkData, context);
  }

  /**
   * ğŸªŸ Sliding window tool call detection - Handle various non-standard formats
   */
  private async slidingWindowToolDetection(data: any, context: PreprocessingContext): Promise<{
    hasTools: boolean;
    toolCount: number;
    patterns: string[];
  }> {
    let toolCount = 0;
    const detectedPatterns: string[] = [];

    // å®šä¹‰æ»‘åŠ¨çª—å£å¤§å°å’Œé‡å 
    const windowSize = 500; // 500å­—ç¬¦çª—å£
    const overlap = 100;    // 100å­—ç¬¦é‡å 

    // æ”¶é›†æ‰€æœ‰æ–‡æœ¬Content
    let allText = '';
    if (data.content && Array.isArray(data.content)) {
      allText = data.content
        .filter((block: any) => block.type === 'text' && block.text)
        .map((block: any) => block.text)
        .join(' ');
    } else if (typeof data === 'string') {
      allText = data;
    }

    if (!allText || allText.length === 0) {
      return { hasTools: false, toolCount: 0, patterns: [] };
    }

    // ğŸªŸ æ»‘åŠ¨çª—å£æ‰«æ
    for (let i = 0; i <= allText.length - windowSize; i += (windowSize - overlap)) {
      const window = allText.substring(i, i + windowSize);
      const windowResult = this.analyzeWindowForTools(window, i);
      
      toolCount += windowResult.toolCount;
      detectedPatterns.push(...windowResult.patterns);

      // å¦‚æœçª—å£å¤ªå°ï¼Œç›´æ¥å¤„ç†å‰©ä½™éƒ¨åˆ†
      if (i + windowSize >= allText.length) {
        break;
      }
    }

    // å¤„ç†æœ€åä¸€ä¸ªçª—å£ï¼ˆå¦‚æœæœ‰å‰©ä½™ï¼‰
    if (allText.length > windowSize) {
      const lastWindow = allText.substring(Math.max(0, allText.length - windowSize));
      const lastResult = this.analyzeWindowForTools(lastWindow, allText.length - windowSize);
      toolCount += lastResult.toolCount;
      detectedPatterns.push(...lastResult.patterns);
    }

    return {
      hasTools: toolCount > 0,
      toolCount,
      patterns: [...new Set(detectedPatterns)] // å»é‡
    };
  }

  /**
   * Analyze tool calls in individual windows
   */
  private analyzeWindowForTools(window: string, offset: number): {
    toolCount: number;
    patterns: string[];
  } {
    let toolCount = 0;
    const patterns: string[] = [];

    // æ£€æµ‹æ¨¡å¼1: GLM-4.5Format "Tool call: FunctionName({...})"
    const glmPattern = /Tool\s+call:\s*(\w+)\s*\((\{[^}]*\})\)/gi;
    let match;
    while ((match = glmPattern.exec(window)) !== null) {
      toolCount++;
      patterns.push(`GLM-${match[1]}@${offset + match.index}`);
    }

    // æ£€æµ‹æ¨¡å¼2: JSONFormat {"type": "tool_use", ...}
    const jsonPattern = /\{\s*"type"\s*:\s*"tool_use"[^}]*\}/gi;
    while ((match = jsonPattern.exec(window)) !== null) {
      toolCount++;
      patterns.push(`JSON-tool_use@${offset + match.index}`);
    }

    // æ£€æµ‹æ¨¡å¼3: ç›´æ¥å‡½æ•°è°ƒç”¨Format "functionName({...})"
    const funcPattern = /(\w+)\s*\(\s*\{[^}]*"[^"]*"\s*:[^}]*\}/gi;
    while ((match = funcPattern.exec(window)) !== null) {
      // æ’é™¤å¸¸è§çš„éTool callæ¨¡å¼
      const funcName = match[1].toLowerCase();
      if (!['console', 'json', 'object', 'array', 'string', 'math'].includes(funcName)) {
        toolCount++;
        patterns.push(`FUNC-${match[1]}@${offset + match.index}`);
      }
    }

    // æ£€æµ‹æ¨¡å¼4: OpenAIå‡½æ•°è°ƒç”¨Format
    const openaiPattern = /"function_call"\s*:\s*\{[^}]*"name"\s*:\s*"([^"]+)"/gi;
    while ((match = openaiPattern.exec(window)) !== null) {
      toolCount++;
      patterns.push(`OPENAI-${match[1]}@${offset + match.index}`);
    }

    return { toolCount, patterns };
  }

  /**
   * æ ¸å¿ƒç»Ÿä¸€å¤„ç†æµæ°´çº¿
   * é›†æˆæ‰€æœ‰è¡¥ä¸æ£€æµ‹å’Œåº”ç”¨é€»è¾‘
   */
  private async processWithUnifiedPipeline(
    data: any,
    context: PreprocessingContext
  ): Promise<any> {
    const startTime = Date.now();

    try {
      // æ€§èƒ½è·Ÿè¸ªå¼€å§‹
      if (this.config.performanceTracking) {
        this.performanceMetrics.totalProcessed++;
        this.performanceMetrics.byStage[context.stage].count++;
      }

      // 1. Preprocessingæ£€æŸ¥ï¼šæ˜¯å¦å¯ç”¨
      if (!this.config.enabled) {
        if (this.config.debugMode) {
          this.logger.debug('UnifiedPatchPreprocessor disabled, skipping', {
            requestId: context.requestId
          });
        }
        return data;
      }

      // 2. ç¼“å­˜æ£€æŸ¥ï¼ˆå¯é€‰ä¼˜åŒ–ï¼‰
      if (this.config.cacheResults) {
        const cacheKey = this.generateCacheKey(data, context);
        const cached = this.processedCache.get(cacheKey);
        if (cached) {
          this.logger.debug('[TRACE] Cache hit for preprocessing', {
            requestId: context.requestId,
            cacheKey
          });
          return cached;
        }
      }

      // 3. æ”¾å®½å‡†å…¥æ¡ä»¶ - å¼ºåˆ¶æ‰€æœ‰Responseéƒ½è¿›å…¥Preprocessing
      const shouldProcess = this.config.forceAllInputs || 
                           context.stage === 'response' ||  // æ‰€æœ‰Responseéƒ½è¿›å…¥Preprocessing 
                           this.shouldProcess(data, context);
      
      if (shouldProcess) {
        // æ„å»ºè¡¥ä¸ä¸Šä¸‹æ–‡
        const patchContext: PatchContext = {
          provider: context.provider,
          model: context.model,
          requestId: context.requestId,
          timestamp: context.timestamp
        };

        // åº”ç”¨å¯¹åº”ç±»å‹çš„è¡¥ä¸
        let processedData = data;
        
        if (context.stage === 'input') {
          processedData = await this.patchManager.applyRequestPatches(
            data, 
            context.provider, 
            context.model
          );
        } else if (context.stage === 'response') {
          // ğŸ”§ CRITICAL FIX: ShuaiHong/ModelScope format compatibility patch
          data = await this.applyShuaiHongFormatPatch(data, context);
          
          // ğŸ¯ Force tool call detection and finish reason override
          const toolDetectionResult = await this.forceToolCallDetection(data, context);
          
          if (toolDetectionResult.hasTools) {
            // Force override finish_reason
            data = this.forceFinishReasonOverride(data, 'tool_calls', context);
            console.log(`ğŸ”§ [PREPROCESSING] Forced finish_reason override for ${toolDetectionResult.toolCount} tools`);
          }

          // ğŸš¨ CRITICAL: Detect unknown finish reason in preprocessing stage (å¼ºåˆ¶å¯ç”¨)
          this.validateFinishReason(data, context);
          
          processedData = await this.patchManager.applyResponsePatches(
            data, 
            context.provider, 
            context.model, 
            context.requestId
          );
        } else if (context.stage === 'streaming') {
          processedData = await this.patchManager.applyStreamingPatches(
            data, 
            context.provider, 
            context.model, 
            context.requestId
          );
        }

        // 4. ç¼“å­˜ç»“æœï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if (this.config.cacheResults && processedData !== data) {
          const cacheKey = this.generateCacheKey(data, context);
          this.processedCache.set(cacheKey, processedData);
          
          // é™åˆ¶ç¼“å­˜å¤§å°
          if (this.processedCache.size > 1000) {
            const firstKey = this.processedCache.keys().next().value;
            if (firstKey !== undefined) {
              this.processedCache.delete(firstKey);
            }
          }
        }

        // 5. æ€§èƒ½è·Ÿè¸ªå’Œè°ƒè¯•æ—¥å¿—
        const duration = Date.now() - startTime;
        if (this.config.performanceTracking) {
          this.performanceMetrics.totalDuration += duration;
          this.performanceMetrics.byStage[context.stage].duration += duration;
        }

        if (this.config.debugMode && processedData !== data) {
          this.logger.debug('UnifiedPatchPreprocessor applied changes', {
            requestId: context.requestId,
            stage: context.stage,
            provider: context.provider,
            model: context.model,
            duration: `${duration}ms`,
            dataChanged: true
          });
        }

        return processedData;
      }

      // 6. æ•°æ®æœªå¤„ç†çš„æƒ…å†µ
      if (this.config.debugMode) {
        this.logger.debug('[TRACE] Data bypassed UnifiedPatchPreprocessor', {
          requestId: context.requestId,
          stage: context.stage,
          reason: 'No matching conditions'
        });
      }

      return data;

    } catch (error) {
      // ç»Ÿä¸€é”™è¯¯å¤„ç†ï¼šè¡¥ä¸å¤±è´¥ä¸åº”è¯¥é˜»æ–­ä¸»æµç¨‹
      this.logger.error('UnifiedPatchPreprocessor error', {
        error: error instanceof Error ? error.message : String(error),
        requestId: context.requestId,
        stage: context.stage,
        provider: context.provider,
        model: context.model
      });

      // è¿”å›åŸå§‹æ•°æ®ï¼Œç¡®ä¿ç³»ç»Ÿç»§ç»­è¿è¡Œ
      return data;
    }
  }

  /**
   * ğŸ¯ Force tool call detection - Cannot be disabled by configuration
   * ä½¿ç”¨æ»‘åŠ¨çª—å£è§£æå„ç§ä¸è§„èŒƒFormat
   */
  private async forceToolCallDetection(data: any, context: PreprocessingContext): Promise<{
    hasTools: boolean;
    toolCount: number;
  }> {
    let hasTools = false;
    let toolCount = 0;

    // ğŸªŸ Sliding window parsing mechanism - æ£€æµ‹å„ç§ä¸è§„èŒƒçš„Tool callFormat
    if (data && typeof data === 'object') {
      const slidingWindowResult = await this.slidingWindowToolDetection(data, context);
      hasTools = slidingWindowResult.hasTools;
      toolCount = slidingWindowResult.toolCount;

      if (hasTools && this.config.debugMode) {
        this.logger.debug(`ğŸªŸ [SLIDING-WINDOW] Detected ${toolCount} tools using sliding window analysis`, {
          requestId: context.requestId,
          patterns: slidingWindowResult.patterns
        });
      }
    }

    // 1. æ£€æŸ¥AnthropicFormatçš„Tool call
    if (data.content && Array.isArray(data.content)) {
      const directToolCalls = data.content.filter((block: any) => block.type === 'tool_use');
      toolCount += directToolCalls.length;

      // æ£€æŸ¥Text format tool calls
      const textBlocks = data.content.filter((block: any) => block.type === 'text');
      for (const block of textBlocks) {
        if (block.text && this.hasTextToolCallsSimplified(block.text)) {
          toolCount++;
        }
      }
    }

    // 2. Check OpenAI format tool calls
    if (data.choices && Array.isArray(data.choices)) {
      for (const choice of data.choices) {
        if (choice.message?.tool_calls) {
          toolCount += choice.message.tool_calls.length;
        }
        if (choice.message?.function_call) {
          toolCount++;
        }
      }
    }

    // 3. æ£€æŸ¥GeminiFormatçš„Tool call
    if (data.candidates && Array.isArray(data.candidates)) {
      for (const candidate of data.candidates) {
        if (candidate.content?.parts) {
          const toolParts = candidate.content.parts.filter((part: any) => 
            part.functionCall || part.function_call
          );
          toolCount += toolParts.length;
        }
      }
    }

    hasTools = toolCount > 0;

    if (hasTools) {
      console.log(`ğŸ” [PREPROCESSING] Tool detection result: ${toolCount} tools found in ${context.provider} response`);
    }

    return { hasTools, toolCount };
  }

  /**
   * Simplified text tool call detection
   */
  private hasTextToolCallsSimplified(text: string): boolean {
    const simpleToolPatterns = [
      /Tool\s+call:\s*\w+\s*\(/i,  // GLM-4.5Format
      /"type"\s*:\s*"tool_use"/i,   // JSONFormat
      /"name"\s*:\s*"\w+"/i         // å·¥å…·åç§°
    ];

    return simpleToolPatterns.some(pattern => pattern.test(text));
  }

  /**
   * Solve OpenAI response missing choices error and tool call parsing issue
   */
  private async applyShuaiHongFormatPatch(
    data: any, 
    context: PreprocessingContext
  ): Promise<any> {
    // Handle ShuaiHong/ModelScope format responses
    if (data.message && typeof data.message === 'string') return data.message;
    if (data.text) return data.text;
    if (data.response) return data.response;
    if (data.output) return data.output;
    
    // Try to extract from nested objects
    if (data.result && data.result.content) return data.result.content;
    if (data.data && data.data.content) return data.data.content;
    
    // LM Studio special handling: Parse embedded tool calls in content
    const isLMStudio = context.provider.includes('lmstudio') || context.provider.includes('LMStudio');
    
    if (isLMStudio) {
      // Handle both OpenAI format (choices array) and Anthropic format (content array)
      let textContent = '';
      let isOpenAIFormat = false;
      
      // Check for OpenAI format first
      if (data.choices && Array.isArray(data.choices) && data.choices.length > 0) {
        const choice = data.choices[0];
        textContent = choice.message?.content;
        isOpenAIFormat = true;
      }
      // Check for Anthropic format
      else if (data.content && Array.isArray(data.content)) {
        const textBlock = data.content.find((block: any) => block.type === 'text');
        textContent = textBlock?.text;
        isOpenAIFormat = false;
      }
      
      if (typeof textContent === 'string' && textContent.length > 0) {
        // Try to parse LM Studio format tool calls
        const lmstudioToolCalls = this.parseLMStudioToolCalls(textContent, context);
        
        if (lmstudioToolCalls.length > 0) {
          console.log(`ğŸ”§ [PREPROCESSING] Parsed ${lmstudioToolCalls.length} LM Studio tool calls (${isOpenAIFormat ? 'OpenAI' : 'Anthropic'} format)`);
          
          // Remove tool call markers from content
          let newContent = textContent;
          const lmstudioPattern = /<\|start\|>assistant<\|channel\|>commentary to=functions\.[^<]*\s*<\|constrain\|>[^<]*<\|message\|>\{[^}]*\}/g;
          newContent = newContent.replace(lmstudioPattern, '').trim();
          
          if (isOpenAIFormat) {
            // Return OpenAI format
            const choice = data.choices[0];
            const fixedData = {
              ...data,
              choices: [{
                ...choice,
                message: {
                  ...choice.message,
                  content: newContent || null,
                  tool_calls: lmstudioToolCalls
                },
                finish_reason: 'tool_calls'
              }]
            };
            
            return fixedData;
          } else {
            // Return Anthropic format with tool_use blocks
            const toolUseBlocks = lmstudioToolCalls.map(toolCall => ({
              type: 'tool_use',
              id: toolCall.id,
              name: toolCall.function.name,
              input: JSON.parse(toolCall.function.arguments)
            }));
            
            const newContentBlocks = [];
            if (newContent) {
              newContentBlocks.push({
                type: 'text',
                text: newContent
              });
            }
            newContentBlocks.push(...toolUseBlocks);
            
            const fixedData = {
              ...data,
              content: newContentBlocks,
              stop_reason: 'tool_use'
            };
            
            return fixedData;
          }
        }
      }
    }
    
    return data;
  }

  /**
   * Extract tool calls from non-standard response
   */
  private extractToolCalls(data: any): any[] | null {
    // Check standard locations
    if (data.tool_calls && Array.isArray(data.tool_calls)) {
      return data.tool_calls;
    }
    
    // Check nested locations
    if (data.message && data.message.tool_calls) {
      return data.message.tool_calls;
    }
    
    // æ£€æŸ¥å…¶ä»–å¯èƒ½çš„ä½ç½®
    if (data.function_calls) {
      return data.function_calls;
    }
    
    return null;
  }

  /**
   * Extract finish_reason from non-standard response
   */
  private extractFinishReason(data: any): string {
    // Try multiple possible finish_reason fields
    if (data.finish_reason) return data.finish_reason;
    if (data.stop_reason) return data.stop_reason;
    if (data.finishReason) return data.finishReason;
    if (data.status) return data.status;
    
    // Check nested locations
    if (data.result && data.result.finish_reason) return data.result.finish_reason;
    if (data.choices && data.choices[0] && data.choices[0].finish_reason) {
      return data.choices[0].finish_reason;
    }
    
    // å¦‚æœæœ‰Tool callç›¸å…³Contentï¼Œè¿”å›tool_calls
    if (this.extractToolCalls(data)) {
      return 'tool_calls';
    }
    
    // é»˜è®¤ä¸ºstop
    return 'stop';
  }

  /**
   * è§£æLM StudioFormatçš„Tool call
   */
  private parseLMStudioToolCalls(content: string, context: PreprocessingContext): any[] {
    const toolCalls: any[] = [];
    
    // LM StudioFormat: <|start|>assistant<|channel|>commentary to=functions.FunctionName <|constrain|>JSON<|message|>{"param":"value"}
    const lmstudioPattern = /<\|start\|>assistant<\|channel\|>commentary to=functions\.(\w+)\s*<\|constrain\|>(?:JSON|json)<\|message\|>(\{[^}]*\})/g;
    
    let match;
    while ((match = lmstudioPattern.exec(content)) !== null) {
      try {
        const functionName = match[1];
        const argsJson = match[2];
        const args = JSON.parse(argsJson);
        
        const toolCall = {
          id: `call_${Date.now()}_${toolCalls.length}`,
          type: 'function',
          function: {
            name: functionName,
            arguments: JSON.stringify(args)
          }
        };
        
        toolCalls.push(toolCall);
        
        this.logger.info('Parsed LM Studio tool call', {
          functionName,
          args,
          provider: context.provider,
          model: context.model,
          requestId: context.requestId
        }, context.requestId, 'preprocessing');
      } catch (error) {
        this.logger.error('Failed to parse LM Studio tool call', {
          error: error instanceof Error ? error.message : String(error),
          match: match[0],
          provider: context.provider,
          model: context.model,
          requestId: context.requestId
        }, context.requestId, 'preprocessing');
      }
    }
    
    return toolCalls;
  }

  /**
   * ğŸ¯ å¼ºåˆ¶finish reasonè¦†ç›–
   */
  private forceFinishReasonOverride(
    data: any, 
    targetReason: string, 
    context: PreprocessingContext
  ): any {
    const originalData = JSON.parse(JSON.stringify(data)); // æ·±æ‹·è´

    // æ ¹æ®ä¸åŒFormatè¿›è¡Œè¦†ç›–
    if (data.choices && Array.isArray(data.choices)) {
      // OpenAIFormat
      for (const choice of data.choices) {
        const originalReason = choice.finish_reason;
        choice.finish_reason = targetReason;
        console.log(`ğŸ”§ [PREPROCESSING] OpenAI format finish_reason: ${originalReason} -> ${targetReason}`);
      }
    }

    if (data.stop_reason !== undefined) {
      // AnthropicFormat
      const originalReason = data.stop_reason;
      data.stop_reason = targetReason === 'tool_calls' ? 'tool_use' : targetReason;
      console.log(`ğŸ”§ [PREPROCESSING] Anthropic format stop_reason: ${originalReason} -> ${data.stop_reason}`);
    }

    return data;
  }

  /**
   * ğŸš¨ CRITICAL: éªŒè¯Responseæœ‰æ•ˆæ€§ - åœ¨Preprocessingé˜¶æ®µæ•è·å¼‚å¸¸Response
   * æŒ‰ç…§ç”¨æˆ·è¦æ±‚ï¼šå…ˆæ£€æŸ¥æ˜¯å¦ä¸ºéNormal responseï¼Œå¦‚æœæ˜¯åˆ™è¿”å›é”™è¯¯ç å’Œæè¿°
   * åªæœ‰Only process finish_reason for normal cases like HTTP 200
   */
  private validateFinishReason(data: any, context: PreprocessingContext): void {
    // 1ï¸âƒ£ é¦–å…ˆæ£€æŸ¥æ˜¯å¦ä¸ºéæ­£å¸¸çš„APIResponse
    const abnormalResponse = this.detectAbnormalResponse(data, context);
    if (abnormalResponse) {
      // æ ¹æ®ç”¨æˆ·è¦æ±‚ï¼šAbnormal response directly throws API errorï¼ŒåŒ…å«é”™è¯¯ç å’Œ500å­—ä»¥å†…æè¿°
      const errorCode = abnormalResponse.statusCode || 500;
      const errorMessage = this.generateErrorMessage(abnormalResponse, context);
      
      this.logger.error('ğŸš¨ [PREPROCESSING] Abnormal API response detected - RETURNING API ERROR', {
        errorCode,
        errorMessage: errorMessage.slice(0, 500),
        provider: context.provider,
        model: context.model,
        requestId: context.requestId,
        abnormalType: abnormalResponse.type,
        stage: 'preprocessing-validation',
        action: 'RETURN_API_ERROR'
      }, context.requestId, 'preprocessing');
      
      // æŠ›å‡ºç»“æ„åŒ–çš„Provideré”™è¯¯
      const error = new Error(errorMessage) as any;
      error.statusCode = errorCode;
      error.provider = context.provider;
      error.model = context.model;
      error.requestId = context.requestId;
      error.stage = 'provider';
      error.timestamp = new Date().toISOString();
      error.details = {
        diagnostics: abnormalResponse.diagnosis
      };
      
      throw error;
    }
    
    // 2ï¸âƒ£ å¯¹äºHTTP 200ç­‰æ­£å¸¸æƒ…å†µï¼Œæ£€æŸ¥finish_reason
    this.validateNormalResponseFinishReason(data, context);
  }

  /**
   * Detect abnormal API responses
   * Based on user diagnosis: ModelScope not sending finish_reason field situation
   */
  private detectAbnormalResponse(data: any, context: PreprocessingContext): {
    type: string;
    statusCode: number;
    diagnosis: string;
  } | null {
    // æ£€æŸ¥ModelScopeç‰¹å®šçš„å¼‚å¸¸æƒ…å†µï¼šstreamç»“æŸä½†æ²¡æœ‰finish_reason
    if (this.isModelScopeProvider(context.provider) && this.isStreamingEndWithoutFinishReason(data, context)) {
      return {
        type: 'missing_finish_reason',
        statusCode: 500,
        diagnosis: 'Silent failure detected and fixed'
      };
    }
    
    // æ£€æŸ¥ç©ºResponseæˆ–æ— æ•ˆResponseFormat
    if (!data || (typeof data === 'object' && Object.keys(data).length === 0)) {
      return {
        type: 'empty_response',
        statusCode: 502,
        diagnosis: 'Provider returned empty response'
      };
    }
    
    // æ£€æŸ¥HTTPé”™è¯¯Response
    if (data.error || data.status >= 400) {
      return {
        type: 'http_error',
        statusCode: data.status || 500,
        diagnosis: 'Provider returned HTTP error response'
      };
    }
    
    // æ£€æŸ¥timeoutæˆ–connectioné”™è¯¯
    if (data.code === 'ETIMEDOUT' || data.code === 'ECONNREFUSED') {
      return {
        type: 'connection_error',
        statusCode: 503,
        diagnosis: 'Provider connection or timeout error'
      };
    }
    
    return null;
  }

  /**
   * Detect if provider is ModelScope type
   */
  private isModelScopeProvider(provider: string): boolean {
    return Boolean(provider && (
      provider.toLowerCase().includes('modelscope') || 
      provider.toLowerCase().includes('qwen') ||
      provider.includes('openai-key2')  // æ ¹æ®é”™è¯¯ä¿¡æ¯ä¸­çš„providerå
    ));
  }

  /**
   * Detect if stream ends but finish_reason is missing
   * åŸºäºè¯Šæ–­æµ‹è¯•å‘ç°ï¼šModelScopeè¿”å›message_deltaä½†deltaä¸ºç©ºå¯¹è±¡
   */
  private isStreamingEndWithoutFinishReason(data: any, context: PreprocessingContext): boolean {
    // æ£€æŸ¥OpenAIFormatResponseä¸­çš„ç©ºfinish_reason
    if (data && typeof data === 'object' && 'choices' in data) {
      const choices = data.choices;
      if (Array.isArray(choices) && choices.length > 0) {
        const choice = choices[0];
        // å¦‚æœæœ‰messageä½†æ²¡æœ‰finish_reasonï¼Œä¸”usageè¡¨æ˜å·²å®Œæˆï¼Œåˆ™åˆ¤å®šä¸ºå¼‚å¸¸
        if (choice.message && !choice.finish_reason && data.usage?.output_tokens > 0) {
          return true;
        }
      }
    }
    
    // æ£€æŸ¥æµäº‹ä»¶Formatï¼šmessage_deltaä½†deltaä¸ºç©º
    if (data && data.type === 'message_delta' && data.delta && Object.keys(data.delta).length === 0) {
      return true;
    }
    
    return false;
  }

  /**
   * Generate friendly error message (limited to 500 characters)
   */
  private generateErrorMessage(abnormalResponse: any, context: PreprocessingContext): string {
    const baseMessage = `Provider returned unknown finish reason, indicating connection or API issue.`;
    const details = `Provider: ${context.provider}, Model: ${context.model}`;
    
    switch (abnormalResponse.type) {
      case 'missing_finish_reason':
        return `${baseMessage} ${details}. The provider completed the response but failed to send proper completion status. This typically indicates a provider API compatibility issue or temporary service disruption.`;
      
      case 'empty_response':
        return `${baseMessage} ${details}. Provider returned empty response, indicating connection timeout or service unavailability.`;
      
      case 'http_error':
        return `${baseMessage} ${details}. Provider returned HTTP error ${abnormalResponse.statusCode}.`;
      
      case 'connection_error':
        return `${baseMessage} ${details}. Network connection failed or timed out.`;
      
      default:
        return `${baseMessage} ${details}. Unexpected provider response format detected.`;
    }
  }

  /**
   * Validate finish_reason in normal response (original logic remains unchanged)
   */
  private validateNormalResponseFinishReason(data: any, context: PreprocessingContext): void {
    // æ£€æŸ¥OpenAIFormatçš„Response
    if (data && typeof data === 'object' && 'choices' in data) {
      const choices = data.choices;
      if (Array.isArray(choices) && choices.length > 0) {
        const finishReason = choices[0].finish_reason;
        
        // Record original finish reason
        this.logger.info('ğŸ” [PREPROCESSING] Raw finish_reason detected', {
          originalFinishReason: finishReason,
          provider: context.provider,
          model: context.model,
          requestId: context.requestId,
          stage: 'preprocessing-validation',
          timestamp: new Date().toISOString()
        }, context.requestId, 'preprocessing');
        
        // For normal responses, only process when explicitly "unknown"
        if (finishReason === 'unknown') {
          if (this.config.strictFinishReasonValidation) {
            const error = new Error(`Provider returned explicit unknown finish_reason. Provider: ${context.provider}, Model: ${context.model}`);
            this.logger.error('ğŸš¨ [PREPROCESSING] Explicit unknown finish_reason in normal response', {
              originalFinishReason: finishReason,
              provider: context.provider,
              model: context.model,
              requestId: context.requestId,
              error: error.message,
              strictMode: true
            }, context.requestId, 'preprocessing');
            throw error;
          }
        }
        
        // Record valid finish reason
        this.logger.debug('âœ… [PREPROCESSING] Valid finish_reason confirmed', {
          finishReason,
          provider: context.provider,
          model: context.model,
          requestId: context.requestId
        }, context.requestId, 'preprocessing');
      }
    }
  }

  /**
   * Smart detection: Determine if data needs processing
   */
  private shouldProcess(data: any, context: PreprocessingContext): boolean {
    // å¦‚æœå¼ºåˆ¶å¤„ç†æ‰€æœ‰è¾“å…¥ï¼Œç›´æ¥è¿”å›true
    if (this.config.forceAllInputs) {
      return true;
    }

    // Check bypass conditions
    for (const bypass of this.config.bypassConditions) {
      if (this.matchesCondition(data, context, bypass)) {
        return false;
      }
    }

    // åŸºæœ¬æ£€æµ‹è§„åˆ™ï¼š
    // 1. è¾“å…¥é˜¶æ®µï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«éœ€è¦å¤„ç†çš„Format
    // 2. Responseé˜¶æ®µï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«Tool callæˆ–ç‰¹æ®ŠFormat
    // 3. Streamingé˜¶æ®µï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«éœ€è¦ä¿®å¤çš„æ•°æ®å—

    switch (context.stage) {
      case 'input':
        return this.detectInputProcessingNeeded(data, context);
      case 'response':
        return this.detectResponseProcessingNeeded(data, context);
      case 'streaming':
        return this.detectStreamingProcessingNeeded(data, context);
      default:
        return false;
    }
  }

  /**
   * Detect if input needs preprocessing
   */
  private detectInputProcessingNeeded(data: any, context: PreprocessingContext): boolean {
    // æ£€æŸ¥æ˜¯å¦åŒ…å«Tool callç›¸å…³çš„Content
    if (data && typeof data === 'object') {
      // æ£€æŸ¥ tools Field
      if (data.tools && Array.isArray(data.tools)) {
        return true;
      }

      // Check if messages contain tool call content
      if (data.messages && Array.isArray(data.messages)) {
        return data.messages.some((msg: any) => {
          if (typeof msg.content === 'string') {
            return /tool_use|Tool call|function/i.test(msg.content);
          }
          return false;
        });
      }
    }

    return false;
  }

  /**
   * Detect if response needs preprocessing
   */
  private detectResponseProcessingNeeded(data: any, context: PreprocessingContext): boolean {
    if (!data || typeof data !== 'object') {
      return false;
    }

    // Check if text format tool calls are included
    if (data.content && Array.isArray(data.content)) {
      return data.content.some((block: any) => {
        if (block.type === 'text' && typeof block.text === 'string') {
          return /Tool call:|tool_use|function_call/i.test(block.text);
        }
        return false;
      });
    }

    // Check OpenAI format tool calls
    if (data.choices && Array.isArray(data.choices)) {
      return data.choices.some((choice: any) => {
        return choice.message && (choice.message.tool_calls || choice.message.function_call);
      });
    }

    // æ£€æŸ¥GeminiFormat
    if (data.candidates && Array.isArray(data.candidates)) {
      return true; // GeminiResponseéƒ½éœ€è¦Formatä¿®å¤
    }

    return false;
  }

  /**
   * Detect if streaming data needs preprocessing
   */
  private detectStreamingProcessingNeeded(data: any, context: PreprocessingContext): boolean {
    if (!data || typeof data !== 'object') {
      return false;
    }

    // Check tool calls in streaming events
    if (data.event && data.data) {
      const eventType = data.event;
      const eventData = data.data;

      if (eventType === 'content_block_start' && eventData.content_block?.type === 'tool_use') {
        return true;
      }

      if (eventType === 'content_block_delta' && eventData.delta?.text) {
        return /Tool call:|tool_use/i.test(eventData.delta.text);
      }
    }

    return false;
  }

  /**
   * ç”Ÿæˆç¼“å­˜é”®
   */
  private generateCacheKey(data: any, context: PreprocessingContext): string {
    const dataHash = JSON.stringify(data).substring(0, 100);
    return `${context.stage}-${context.provider}-${context.model}-${dataHash}`;
  }

  /**
   * æ£€æŸ¥æ˜¯å¦åŒ¹é…ç‰¹å®šæ¡ä»¶
   */
  private matchesCondition(data: any, context: PreprocessingContext, condition: string): boolean {
    // ç®€å•çš„æ¡ä»¶åŒ¹é…é€»è¾‘ï¼Œå¯ä»¥æ ¹æ®éœ€è¦æ‰©å±•
    if (condition === 'no-tools' && !data.tools) {
      return true;
    }

    if (condition.startsWith('provider:')) {
      const providerName = condition.split(':')[1];
      return context.provider === providerName;
    }

    if (condition.startsWith('model:')) {
      const modelName = condition.split(':')[1];
      return context.model.includes(modelName);
    }

    return false;
  }

  /**
   * è·å–æ€§èƒ½ç»Ÿè®¡
   */
  getPerformanceMetrics() {
    const avgDuration = this.performanceMetrics.totalProcessed > 0 
      ? this.performanceMetrics.totalDuration / this.performanceMetrics.totalProcessed 
      : 0;

    return {
      ...this.performanceMetrics,
      averageDuration: Math.round(avgDuration * 100) / 100,
      cacheSize: this.processedCache.size,
      config: this.config
    };
  }

  /**
   * è·å–è¡¥ä¸ç®¡ç†å™¨ç»Ÿè®¡
   */
  getPatchManagerStats() {
    return this.patchManager.getStats();
  }

  /**
   * æ¸…ç†ç¼“å­˜
   */
  clearCache() {
    this.processedCache.clear();
    this.logger.info('UnifiedPatchPreprocessor cache cleared');
  }

  /**
   * é‡ç½®æ€§èƒ½æŒ‡æ ‡
   */
  resetMetrics() {
    this.performanceMetrics = {
      totalProcessed: 0,
      totalDuration: 0,
      byStage: {
        input: { count: 0, duration: 0 },
        response: { count: 0, duration: 0 },
        streaming: { count: 0, duration: 0 }
      }
    };
    this.logger.info('UnifiedPatchPreprocessor metrics reset');
  }

  /**
   * æ›´æ–°é…ç½®
   */
  updateConfig(newConfig: Partial<UnifiedPatchPreprocessorConfig>) {
    this.config = { ...this.config, ...newConfig };
    this.logger.info('UnifiedPatchPreprocessor config updated', { newConfig });
  }
}

// å•ä¾‹æ¨¡å¼ï¼šå…¨å±€ç»Ÿä¸€Preprocessingå™¨å®ä¾‹
const preprocessorInstances = new Map<number | string, UnifiedPatchPreprocessor>();

/**
 * è·å–æˆ–åˆ›å»ºç»Ÿä¸€Preprocessingå™¨å®ä¾‹
 */
export function getUnifiedPatchPreprocessor(
  port?: number, 
  config?: Partial<UnifiedPatchPreprocessorConfig>
): UnifiedPatchPreprocessor {
  const key = port || 'default';
  
  if (!preprocessorInstances.has(key)) {
    preprocessorInstances.set(key, new UnifiedPatchPreprocessor(port, config));
  }

  return preprocessorInstances.get(key)!;
}

/**
 * åˆ›å»ºæ–°çš„ç»Ÿä¸€Preprocessingå™¨å®ä¾‹
 */
export function createUnifiedPatchPreprocessor(
  port?: number,
  config?: Partial<UnifiedPatchPreprocessorConfig>
): UnifiedPatchPreprocessor {
  const key = port || 'default';
  const instance = new UnifiedPatchPreprocessor(port, config);
  preprocessorInstances.set(key, instance);
  return instance;
}

/**
 * é‡ç½®ç»Ÿä¸€Preprocessingå™¨å®ä¾‹
 */
export function resetUnifiedPatchPreprocessor(port?: number) {
  const key = port || 'default';
  if (preprocessorInstances.has(key)) {
    preprocessorInstances.delete(key);
  }
}