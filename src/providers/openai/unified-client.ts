/**
 * Unified OpenAI Client - ç»Ÿä¸€OpenAIå®¢æˆ·ç«¯å®ç°
 * é¡¹ç›®æ‰€æœ‰è€…: Jason Zhang
 * 
 * åˆå¹¶Pure Clientå’ŒSDK Clientçš„åŠŸèƒ½ï¼Œæ¶ˆé™¤é‡å¤ä»£ç 
 * éµå¾ªé›¶ç¡¬ç¼–ç ã€é›¶Fallbackã€é›¶æ²‰é»˜å¤±è´¥åŸåˆ™
 * ç¬¦åˆå…­å±‚æ¸…æ™°æ¶æ„ï¼šProvideråªåšAPIè°ƒç”¨ï¼Œæ‰€æœ‰è½¬æ¢åœ¨Transformerå±‚
 */

import OpenAI from 'openai';
import { BaseRequest, BaseResponse, Provider, ProviderConfig, ProviderError } from '@/types';
import { logger } from '@/utils/logger';
import { getSimpleSessionManager } from '@/session/simple-session-manager';
import { 
  validateNonStreamingResponse, 
  handleProviderError 
} from '@/utils/response-validation';
import { 
  validateProviderConfig, 
  type ValidatedConfig 
} from '@/utils/config-validation';

export interface UnifiedOpenAIConfig extends ProviderConfig {
  clientType?: 'sdk' | 'pure';
  sdkOptions?: {
    timeout?: number;
    maxRetries?: number;
    defaultHeaders?: Record<string, string>;
  };
}

/**
 * ç»Ÿä¸€OpenAIå®¢æˆ·ç«¯ - åˆå¹¶é‡å¤å®ç°
 * åŸºäºé¡¹ç›®è®°å¿†ä¸­çš„æˆåŠŸé‡æ„ç»éªŒ
 */
export class UnifiedOpenAIClient implements Provider {
  public readonly name: string;
  public readonly type = 'openai-unified';
  public readonly config: UnifiedOpenAIConfig;
  
  private openaiClient: OpenAI;
  private validatedConfig: ValidatedConfig;
  private sessionManager: ReturnType<typeof getSimpleSessionManager>;

  constructor(config: UnifiedOpenAIConfig, providerId: string) {
    this.name = providerId;
    this.config = config;
    
    // ğŸš¨ ä¸¥æ ¼é…ç½®éªŒè¯ - é›¶fallbackåŸåˆ™
    this.validatedConfig = validateProviderConfig(config, providerId, config.sdkOptions);

    // åˆå§‹åŒ–ç»Ÿä¸€OpenAIå®¢æˆ·ç«¯
    this.openaiClient = new OpenAI({
      apiKey: this.validatedConfig.apiKey,
      baseURL: this.validatedConfig.baseURL,
      timeout: this.validatedConfig.sdkOptions.timeout,
      maxRetries: this.validatedConfig.sdkOptions.maxRetries,
      defaultHeaders: this.validatedConfig.sdkOptions.defaultHeaders
    });

    // åˆå§‹åŒ–ä¼šè¯ç®¡ç†
    this.sessionManager = getSimpleSessionManager(this.validatedConfig.port);

    logger.info('Unified OpenAI Client initialized', {
      providerId,
      baseURL: this.validatedConfig.baseURL,
      hasApiKey: true,
      port: this.validatedConfig.port,
      clientType: config.clientType || 'unified',
      architecture: 'six-layer-clean'
    });
  }

  /**
   * ğŸ¯ å‘é€éæµå¼è¯·æ±‚ - ç»Ÿä¸€å®ç°
   * å‡è®¾request.metadata.openaiRequeståŒ…å«å·²è½¬æ¢çš„OpenAIæ ¼å¼è¯·æ±‚
   */
  async sendRequest(request: BaseRequest): Promise<BaseResponse> {
    const originalRequestId = request.metadata?.requestId || 'unknown';
    const sessionId = request.metadata?.sessionId;
    const conversationId = request.metadata?.conversationId;

    // ç”Ÿæˆä¼šè¯IDï¼ˆå¦‚æœæœ‰ä¼šè¯ä¿¡æ¯ï¼‰
    let requestId = originalRequestId;
    if (sessionId && conversationId) {
      requestId = this.sessionManager.generateRequestId(sessionId, conversationId, false);
      request.metadata = { ...request.metadata, requestId, originalRequestId };
    }

    try {
      // ğŸ” ä»metadataä¸­è·å–å·²è½¬æ¢çš„OpenAIè¯·æ±‚
      const openaiRequest = request.metadata?.openaiRequest;
      if (!openaiRequest) {
        throw new Error('Missing openaiRequest in metadata - transformer step required');
      }

      logger.debug('Unified OpenAI API call', {
        model: openaiRequest.model,
        hasTools: !!(openaiRequest.tools && openaiRequest.tools.length > 0),
        messageCount: openaiRequest.messages?.length || 0,
        requestId,
        provider: this.name
      });

      // ğŸ¯ çº¯ç²¹çš„APIè°ƒç”¨ï¼Œä¸åšè½¬æ¢
      const response = await this.openaiClient.chat.completions.create(openaiRequest);
      
      // ğŸ”„ åŒ…è£…ä¸ºBaseResponseæ ¼å¼è¿›è¡ŒéªŒè¯
      const baseResponse = this.wrapResponse(response, requestId);
      
      // ğŸš¨ ä¸¥æ ¼å“åº”éªŒè¯
      validateNonStreamingResponse(baseResponse, requestId, this.name);

      logger.debug('Unified OpenAI API response received', {
        responseId: response.id,
        model: response.model,
        hasChoices: !!(response.choices && response.choices.length > 0),
        finishReason: response.choices?.[0]?.finish_reason,
        requestId,
        provider: this.name
      });

      // ğŸ”„ è¿”å›åŒ…è£…åçš„BaseResponseï¼Œä¿ç•™åŸå§‹å“åº”åœ¨metadataä¸­
      baseResponse.metadata = {
        ...baseResponse.metadata,
        openaiResponse: response,
        providerId: this.name,
        responseId: response.id
      };

      return baseResponse;

    } catch (error) {
      logger.error('Unified OpenAI API call failed', {
        error: error instanceof Error ? error.message : String(error),
        provider: this.name,
        requestId
      });

      throw handleProviderError(error, requestId, this.name, 'non-streaming');
    }
  }

  /**
   * ğŸ¯ å‘é€æµå¼è¯·æ±‚ - ç»Ÿä¸€å®ç°
   */
  async *sendStreamRequest(request: BaseRequest): AsyncIterable<any> {
    const originalRequestId = request.metadata?.requestId || 'unknown';
    const sessionId = request.metadata?.sessionId;
    const conversationId = request.metadata?.conversationId;

    // ç”Ÿæˆä¼šè¯IDï¼ˆå¦‚æœæœ‰ä¼šè¯ä¿¡æ¯ï¼‰
    let requestId = originalRequestId;
    if (sessionId && conversationId) {
      requestId = this.sessionManager.generateRequestId(sessionId, conversationId, true);
      request.metadata = { ...request.metadata, requestId, originalRequestId };
    }

    try {
      // ğŸ” ä»metadataä¸­è·å–å·²è½¬æ¢çš„OpenAIè¯·æ±‚
      const openaiRequest = request.metadata?.openaiRequest;
      if (!openaiRequest) {
        throw new Error('Missing openaiRequest in metadata - transformer step required');
      }

      // ç¡®ä¿æµå¼æ ‡å¿—
      openaiRequest.stream = true;

      logger.debug('Unified OpenAI streaming API call', {
        model: openaiRequest.model,
        hasTools: !!(openaiRequest.tools && openaiRequest.tools.length > 0),
        requestId,
        provider: this.name
      });

      // ğŸ¯ çº¯ç²¹çš„æµå¼APIè°ƒç”¨
      const streamingRequest = { ...openaiRequest, stream: true };
      const stream = await this.openaiClient.chat.completions.create(streamingRequest) as any;

      for await (const chunk of stream) {
        // ğŸ”„ å°†åŸå§‹chunkæ”¾å…¥metadataï¼Œç”±transformerå¤„ç†è½¬æ¢
        const chunkWithMetadata = {
          ...request,
          metadata: {
            ...request.metadata,
            openaiChunk: chunk,
            providerId: this.name,
            chunkId: chunk.id
          }
        };

        yield chunkWithMetadata;
      }

      logger.debug('Unified OpenAI streaming API completed', {
        requestId,
        provider: this.name
      });

    } catch (error) {
      logger.error('Unified OpenAI streaming API failed', {
        error: error instanceof Error ? error.message : String(error),
        provider: this.name,
        requestId
      });

      throw handleProviderError(error, requestId, this.name, 'streaming');
    }
  }

  /**
   * ğŸ”§ å¥åº·æ£€æŸ¥ - ç»Ÿä¸€å®ç°
   */
  async isHealthy(): Promise<boolean> {
    try {
      const response = await this.openaiClient.chat.completions.create({
        model: this.validatedConfig.defaultModel,
        messages: [{ role: 'user', content: 'health check' }],
        max_tokens: 1,
        temperature: 0
      });

      const isHealthy = !!(response?.choices?.[0]);
      
      logger.debug('Health check completed', {
        provider: this.name,
        healthy: isHealthy,
        model: this.validatedConfig.defaultModel
      });

      return isHealthy;
    } catch (error) {
      logger.warn('Health check failed', {
        provider: this.name,
        error: error instanceof Error ? error.message : String(error)
      });
      return false;
    }
  }

  /**
   * ğŸ¯ è·å–æ”¯æŒçš„æ¨¡å‹ - ç»Ÿä¸€å®ç°
   */
  async getSupportedModels(): Promise<string[]> {
    // ä»åŸå§‹é…ç½®ä¸­è¿”å›æ”¯æŒçš„æ¨¡å‹ï¼Œä¸ç¡¬ç¼–ç 
    if (Array.isArray(this.config.models)) {
      return this.config.models;
    }
    
    // é»˜è®¤è¿”å›é…ç½®çš„é»˜è®¤æ¨¡å‹
    return this.config.defaultModel ? [this.config.defaultModel] : [];
  }

  /**
   * ğŸ”„ åŒ…è£…åŸå§‹å“åº”ä¸ºBaseResponseæ ¼å¼
   */
  private wrapResponse(rawResponse: any, requestId: string): BaseResponse {
    return {
      id: rawResponse.id || `provider-${Date.now()}`,
      model: rawResponse.model,
      role: 'assistant',
      content: rawResponse.choices?.[0]?.message?.content ? 
        [{ type: 'text', text: rawResponse.choices[0].message.content }] : [],
      stop_reason: rawResponse.choices?.[0]?.finish_reason || 'unknown',
      usage: rawResponse.usage,
      metadata: {
        requestId,
        provider: this.name,
        rawResponse // ä¿ç•™åŸå§‹å“åº”ä¾›å¤–éƒ¨è½¬æ¢ä½¿ç”¨
      }
    };
  }
}

/**
 * ğŸ¯ å·¥å‚å‡½æ•°ï¼šåˆ›å»ºç»Ÿä¸€OpenAIå®¢æˆ·ç«¯
 */
export function createUnifiedOpenAIClient(
  config: UnifiedOpenAIConfig, 
  providerId: string
): UnifiedOpenAIClient {
  return new UnifiedOpenAIClient(config, providerId);
}