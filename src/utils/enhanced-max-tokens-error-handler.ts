/**
 * å¢å¼ºç‰ˆMax Tokensé”™è¯¯å¤„ç†å™¨
 * é›†æˆæ™ºèƒ½å¤„ç†æ¨¡å—ï¼Œæä¾›æ»šåŠ¨æˆªæ–­å’Œå‹ç¼©ç­‰é«˜çº§åŠŸèƒ½
 * Project Owner: Jason Zhang
 */

import { logger } from './logger';
import { BaseRequest } from '@/types';
import { MaxTokensErrorHandlingModule, TruncationResult } from './max-tokens-error-handling-module';

export interface EnhancedMaxTokensError extends Error {
  status: number;
  code: string;
  details: {
    finishReason: string;
    provider: string;
    model: string;
    requestId: string;
    usage?: any;
    truncationResult?: TruncationResult;
    autoRetryAvailable?: boolean;
  };
}

export interface MaxTokensHandlingOptions {
  enableAutoRetry: boolean; // æ˜¯å¦å¯ç”¨è‡ªåŠ¨é‡è¯•æœºåˆ¶
  maxRetryAttempts: number; // æœ€å¤§é‡è¯•æ¬¡æ•°
  handlingModulePath?: string; // å¤„ç†æ¨¡å—é…ç½®è·¯å¾„
}

export class EnhancedMaxTokensErrorHandler {
  private handlingModule: MaxTokensErrorHandlingModule;
  private options: MaxTokensHandlingOptions;

  constructor(options: Partial<MaxTokensHandlingOptions> = {}) {
    this.options = {
      enableAutoRetry: true,
      maxRetryAttempts: 2,
      ...options
    };

    this.handlingModule = new MaxTokensErrorHandlingModule(options.handlingModulePath);
  }

  /**
   * æ£€æŸ¥finish_reasonæ˜¯å¦è¡¨ç¤ºè¾¾åˆ°äº†tokené™åˆ¶
   */
  static isMaxTokensReached(finishReason: string): boolean {
    const maxTokensReasons = [
      'max_tokens',    // OpenAIæ ¼å¼
      'length',        // OpenAI length limit
      'max_tokens',    // Anthropicæ ¼å¼
    ];
    
    return maxTokensReasons.includes(finishReason?.toLowerCase());
  }

  /**
   * æ£€æŸ¥Anthropicæ ¼å¼çš„stop_reasonæ˜¯å¦è¡¨ç¤ºè¾¾åˆ°äº†tokené™åˆ¶
   */
  static isMaxTokensStopReason(stopReason: string): boolean {
    const maxTokensStopReasons = [
      'max_tokens',    // Anthropic max tokens
    ];
    
    return maxTokensStopReasons.includes(stopReason?.toLowerCase());
  }

  /**
   * æ™ºèƒ½å¤„ç†max tokensé”™è¯¯ - ä¸»å…¥å£
   * @param response APIå“åº”å¯¹è±¡
   * @param provider æä¾›å•†åç§°
   * @param model æ¨¡å‹åç§°
   * @param requestId è¯·æ±‚ID
   * @param originalRequest åŸå§‹è¯·æ±‚ï¼ˆç”¨äºæ™ºèƒ½å¤„ç†ï¼‰
   * @returns å¤„ç†åçš„è¯·æ±‚æˆ–æŠ›å‡ºé”™è¯¯
   */
  async handleMaxTokensResponse(
    response: any,
    provider: string,
    model: string,
    requestId: string,
    originalRequest?: BaseRequest
  ): Promise<{ shouldRetry: boolean; truncatedRequest?: BaseRequest; error?: EnhancedMaxTokensError }> {
    
    // æ£€æŸ¥æ˜¯å¦è¾¾åˆ°tokené™åˆ¶
    const isMaxTokensReached = this.detectMaxTokensCondition(response);
    
    if (!isMaxTokensReached) {
      return { shouldRetry: false };
    }

    logger.warn('ğŸš¨ [MAX-TOKENS] Token limit reached', {
      provider,
      model,
      requestId,
      finishReason: this.extractFinishReason(response),
      autoRetryEnabled: this.options.enableAutoRetry
    });

    // å¦‚æœå¯ç”¨è‡ªåŠ¨å¤„ç†ä¸”æœ‰åŸå§‹è¯·æ±‚
    if (this.options.enableAutoRetry && originalRequest) {
      try {
        const truncationResult = await this.handlingModule.handleMaxTokensError(
          originalRequest,
          { response, provider, model },
          requestId
        );

        if (truncationResult.success) {
          logger.info('ğŸ”§ [MAX-TOKENS] Auto-truncation successful', {
            originalTokens: truncationResult.originalTokens,
            reducedTokens: truncationResult.reducedTokens,
            reductionPercent: Math.round((1 - truncationResult.reducedTokens / truncationResult.originalTokens) * 100),
            strategy: truncationResult.strategy,
            requestId
          });

          return {
            shouldRetry: true,
            truncatedRequest: truncationResult.truncatedRequest
          };
        }
      } catch (error) {
        logger.error('ğŸš¨ [MAX-TOKENS] Auto-handling failed', error, requestId);
      }
    }

    // åˆ›å»ºå¢å¼ºçš„é”™è¯¯å¯¹è±¡
    const enhancedError = this.createEnhancedMaxTokensError(
      this.extractFinishReason(response),
      provider,
      model,
      requestId,
      response.usage,
      originalRequest ? true : false
    );

    return {
      shouldRetry: false,
      error: enhancedError
    };
  }

  /**
   * æ£€æµ‹max tokensæ¡ä»¶
   */
  private detectMaxTokensCondition(response: any): boolean {
    // æ£€æŸ¥OpenAIæ ¼å¼çš„finish_reason
    if (response.choices?.[0]?.finish_reason) {
      return EnhancedMaxTokensErrorHandler.isMaxTokensReached(response.choices[0].finish_reason);
    }

    // æ£€æŸ¥Anthropicæ ¼å¼çš„stop_reason
    if (response.stop_reason) {
      return EnhancedMaxTokensErrorHandler.isMaxTokensStopReason(response.stop_reason);
    }

    // æ£€æŸ¥unifiedæ ¼å¼
    if (response.finishReason) {
      return EnhancedMaxTokensErrorHandler.isMaxTokensReached(response.finishReason);
    }

    return false;
  }

  /**
   * æå–finish reason
   */
  private extractFinishReason(response: any): string {
    if (response.choices?.[0]?.finish_reason) {
      return response.choices[0].finish_reason;
    }
    if (response.stop_reason) {
      return response.stop_reason;
    }
    if (response.finishReason) {
      return response.finishReason;
    }
    return 'unknown';
  }

  /**
   * åˆ›å»ºå¢å¼ºç‰ˆmax tokensé”™è¯¯
   */
  private createEnhancedMaxTokensError(
    finishReason: string,
    provider: string,
    model: string,
    requestId: string,
    usage?: any,
    autoRetryAvailable?: boolean
  ): EnhancedMaxTokensError {
    const message = autoRetryAvailable
      ? `Request exceeded maximum token limit. Auto-retry with intelligent truncation is available.`
      : `Request exceeded maximum token limit. Please reduce input length or increase max_tokens parameter.`;

    const error = new Error(message) as EnhancedMaxTokensError;
    
    error.status = 429; // ä½¿ç”¨429è¡¨ç¤ºå¯ä»¥é‡è¯•
    error.code = 'MAX_TOKENS_EXCEEDED';
    error.details = {
      finishReason,
      provider,
      model,
      requestId,
      usage,
      autoRetryAvailable
    };
    
    return error;
  }

  /**
   * å…¼å®¹æ€§æ–¹æ³• - æ£€æŸ¥å“åº”å¹¶æŠ›å‡ºé”™è¯¯ï¼ˆå‘åå…¼å®¹ï¼‰
   */
  static checkAndThrowMaxTokensError(
    response: any,
    provider: string,
    model: string,
    requestId: string
  ): void {
    const handler = new EnhancedMaxTokensErrorHandler({ enableAutoRetry: false });
    
    if (handler.detectMaxTokensCondition(response)) {
      throw handler.createEnhancedMaxTokensError(
        handler.extractFinishReason(response),
        provider,
        model,
        requestId,
        response.usage,
        false
      );
    }
  }

  /**
   * æ ¼å¼åŒ–é”™è¯¯å“åº”
   */
  static formatErrorResponse(error: EnhancedMaxTokensError): any {
    return {
      error: {
        type: 'max_tokens_exceeded',
        message: error.message,
        code: error.code,
        details: {
          finish_reason: error.details.finishReason,
          provider: error.details.provider,
          model: error.details.model,
          request_id: error.details.requestId,
          usage: error.details.usage,
          auto_retry_available: error.details.autoRetryAvailable,
          suggestion: error.details.autoRetryAvailable 
            ? 'Request can be automatically retried with intelligent truncation'
            : 'Reduce input length or increase max_tokens parameter'
        }
      }
    };
  }

  /**
   * è·å–å¤„ç†æ¨¡å—é…ç½®
   */
  getHandlingConfig() {
    return this.handlingModule.getConfig();
  }

  /**
   * æ›´æ–°å¤„ç†æ¨¡å—é…ç½®
   */
  updateHandlingConfig(updates: any) {
    this.handlingModule.updateConfig(updates);
  }
}

export default EnhancedMaxTokensErrorHandler;