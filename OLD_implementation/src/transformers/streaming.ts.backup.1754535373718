/**
 * Streaming Response Transformer
 * Handles real-time conversion of streaming responses between formats
 */

import { StreamChunk, MessageTransformer } from './types';
import { logger } from '@/utils/logger';
// import { PipelineDebugger, ToolCallError } from '@/debug/pipeline-debugger'; // å·²è¿ç§»åˆ°ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿ
import { ErrorTracker, ToolCallError } from '../logging';
import { PipelineDebugger, ToolCallErrorClass } from '@/utils/logger';
import { mapFinishReason, mapStopReason } from '@/utils/finish-reason-handler';

export interface StreamTransformOptions {
  sourceFormat: 'openai' | 'anthropic';
  targetFormat: 'openai' | 'anthropic';
  model?: string;
  requestId?: string;
  port?: number;
}

export class StreamingTransformer {
  private messageId: string;
  private model: string;
  private requestId: string;
  private toolCallMap: Map<number, any> = new Map();
  private contentBlockIndex = 0;
  private hasStarted = false;
  private isCompleted = false;
  private pipelineDebugger: PipelineDebugger;
  
  // ğŸ”„ äºŒæ¬¡å·¥å…·è°ƒç”¨å¤„ç†æœºåˆ¶
  private needsReprocessing = false;
  private reprocessBuffer: string[] = [];
  private toolCallDetectionAttempts = 0;
  private maxDetectionAttempts = 2;

  constructor(
    private sourceTransformer: MessageTransformer,
    private targetTransformer: MessageTransformer,
    private options: StreamTransformOptions
  ) {
    this.messageId = `msg_${Date.now()}`;
    // Use the model from options (should be the targetModel from routing)
    this.model = options.model || 'unknown';
    this.requestId = options.requestId || 'unknown';
    // ğŸ”§ ä¿®å¤ç¡¬ç¼–ç ï¼šå¿…é¡»ä»é€‰é¡¹ä¸­è·å–ç«¯å£
    if (!options.port) {
      throw new Error('StreamingTransformer requires explicit port specification - no hardcoded defaults allowed');
    }
    this.pipelineDebugger = new PipelineDebugger(options.port);
    
    logger.debug('StreamingTransformer initialized', {
      model: this.model,
      sourceFormat: options.sourceFormat,
      targetFormat: options.targetFormat,
      requestId: this.requestId
    });
  }

  /**
   * Transform streaming response from OpenAI to Anthropic format
   */
  async *transformOpenAIToAnthropic(stream: ReadableStream): AsyncIterable<string> {
    const reader = stream.getReader();
    const decoder = new TextDecoder();
    let buffer = '';
    let outputTokens = 0;
    let stopReason = undefined; // ç§»é™¤é»˜è®¤åœæ­¢åŸå› 

    try {
      // Send message start event
      if (!this.hasStarted) {
        const messageStartEvent = this.createAnthropicEvent('message_start', {
          type: 'message_start',
          message: {
            id: this.messageId,
            type: 'message',
            role: 'assistant',
            content: [],
            model: this.model,
            // å®Œå…¨ç§»é™¤stop_reasonï¼Œä¿è¯åœæ­¢çš„æƒåŠ›åœ¨æ¨¡å‹è¿™è¾¹
            // stop_reason: null,
            stop_sequence: null,
            usage: { input_tokens: 0, output_tokens: 0 }
          }
        });
        if (messageStartEvent) {
          yield messageStartEvent;
        }

        const pingEvent = this.createAnthropicEvent('ping', { type: 'ping' });
        if (pingEvent) {
          yield pingEvent;
        }
        this.hasStarted = true;
      }

      let hasContentBlock = false;

      while (!this.isCompleted) {
        const { done, value } = await reader.read();
        if (done) break;

        buffer += decoder.decode(value, { stream: true });
        const lines = buffer.split('\n');
        buffer = lines.pop() || '';

        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const data = line.slice(6).trim();
            if (data === '[DONE]') {
              this.isCompleted = true;
              break;
            }

            try {
              const chunk = JSON.parse(data);
              const choice = chunk.choices?.[0];
              if (!choice) continue;
              
              // Track if we handled this chunk
              let handledChunk = false;

              // Handle content deltas - with defensive programming like demo1
              if (choice.delta?.content !== undefined) {
                handledChunk = true;
                
                if (!hasContentBlock) {
                  const startEvent = this.createAnthropicEvent('content_block_start', {
                    type: 'content_block_start',
                    index: 0,
                    content_block: { type: 'text', text: '' }
                  });
                  if (startEvent) {
                    yield startEvent;
                    hasContentBlock = true;
                  }
                }

                // Only yield content delta if there is actual non-empty content
                if (choice.delta.content && choice.delta.content.length > 0) {
                  // Check for tool calls appearing in text content (error condition)
                  this.pipelineDebugger.detectToolCallError(
                    choice.delta.content,
                    this.requestId,
                    'streaming-text-delta',
                    'openai',
                    this.model
                  );
                  
                  const deltaEvent = this.createAnthropicEvent('content_block_delta', {
                    type: 'content_block_delta',
                    index: 0,
                    delta: { type: 'text_delta', text: choice.delta.content }
                  });
                  if (deltaEvent) {
                    yield deltaEvent;
                    outputTokens += Math.ceil(choice.delta.content.length / 4);
                  }
                }
                // Note: for empty content, we don't yield a delta but we still marked it as handled
              }

              // Handle tool calls
              if (choice.delta?.tool_calls) {
                handledChunk = true;
                
                for (const toolCall of choice.delta.tool_calls) {
                  const index = toolCall.index ?? 0;
                  
                  if (!this.toolCallMap.has(index)) {
                    // Close previous content block if exists
                    if (hasContentBlock) {
                      const stopEvent = this.createAnthropicEvent('content_block_stop', {
                        type: 'content_block_stop',
                        index: 0
                      });
                      if (stopEvent) {
                        yield stopEvent;
                      }
                    }

                    const blockIndex = hasContentBlock ? 1 : 0;
                    this.toolCallMap.set(index, {
                      id: toolCall.id || `call_${Date.now()}_${index}`,
                      name: toolCall.function?.name || `tool_${index}`,
                      arguments: '',
                      blockIndex
                    });

                    const toolStartEvent = this.createAnthropicEvent('content_block_start', {
                      type: 'content_block_start',
                      index: blockIndex,
                      content_block: {
                        type: 'tool_use',
                        id: this.toolCallMap.get(index).id,
                        name: this.toolCallMap.get(index).name,
                        input: {}
                      }
                    });
                    if (toolStartEvent) {
                      yield toolStartEvent;
                    }
                  }

                  // Update tool call data
                  const toolCallData = this.toolCallMap.get(index);
                  if (toolCall.function?.arguments) {
                    toolCallData.arguments += toolCall.function.arguments;
                    
                    const toolDeltaEvent = this.createAnthropicEvent('content_block_delta', {
                      type: 'content_block_delta',
                      index: toolCallData.blockIndex,
                      delta: {
                        type: 'input_json_delta',
                        partial_json: toolCall.function.arguments
                      }
                    });
                    if (toolDeltaEvent) {
                      yield toolDeltaEvent;
                    }
                  }
                }
              }

              // Handle finish reason
              if (choice.finish_reason) {
                handledChunk = true;
                const originalFinishReason = choice.finish_reason;
                stopReason = mapFinishReason(originalFinishReason);
                
                // ğŸ†• è®°å½•åŸå§‹OpenAIå’Œè½¬æ¢åçš„Anthropic finish reason
                logger.logDualFinishReason(
                  originalFinishReason,
                  stopReason,
                  this.options.sourceFormat,
                  {
                    model: this.model,
                    responseType: 'streaming',
                    context: 'streaming-openai-to-anthropic',
                    chunkData: choice,
                    conversionMethod: 'mapFinishReason'
                  },
                  this.requestId,
                  'dual-reason-streaming'
                );
                
                // åŒæ—¶è®°å½•åˆ°è°ƒè¯•æ—¥å¿—ç³»ç»Ÿ
                try {
                  const { logFinishReasonDebug } = await import('../utils/finish-reason-debug');
                  logFinishReasonDebug(
                    this.requestId,
                    choice.finish_reason,
                    this.options.sourceFormat,
                    this.model,
                    this.options.port || (this as any).port || (() => { 
                      console.error('âŒ CRITICAL: Port not provided to streaming transformer'); 
                      throw new Error('Port must be provided to streaming transformer - no fallback allowed'); 
                    })(),
                    {
                      mappedStopReason: stopReason,
                      context: 'streaming-transformer',
                      timestamp: new Date().toISOString()
                    }
                  );
                } catch (error) {
                  console.error('Failed to log finish reason debug:', error);
                }
              }
              
              // If we didn't handle this chunk, it means it's just a metadata chunk that doesn't need transformation
              // This is normal and we should NOT yield anything for such chunks
              if (!handledChunk) {
                // This is expected for chunks like: {"delta": {"role": "assistant"}} or {"delta": {}}
                // We simply skip them without yielding anything
              }
              
            } catch (error) {
              logger.debug('Failed to parse streaming chunk', error, this.requestId);
              
              // Check for tool call errors in the problematic chunk
              if (this.isLikelyToolCallError(data, error)) {
                this.pipelineDebugger.logToolCallError(
                  new ToolCallErrorClass(
                    'Tool call parsing error detected',
                    this.requestId,
                    'openai-to-anthropic',
                    'openai',
                    this.model,
                    {
                      rawChunk: data,
                      error: (error as Error).message
                    },
                    this.options.port || (() => { console.error('âŒ CRITICAL: Port not provided to streaming transformer'); throw new Error('Port must be provided to streaming transformer - no fallback allowed'); })()
                  )
                );
                              }
            }
          }
        }
      }

      // Send completion events
      if (hasContentBlock) {
        const stopEvent = this.createAnthropicEvent('content_block_stop', {
          type: 'content_block_stop',
          index: 0
        });
        if (stopEvent) {
          yield stopEvent;
        }
      }

      // Close any open tool call blocks
      for (const [index, toolCall] of this.toolCallMap.entries()) {
        const toolStopEvent = this.createAnthropicEvent('content_block_stop', {
          type: 'content_block_stop',
          index: toolCall.blockIndex
        });
        if (toolStopEvent) {
          yield toolStopEvent;
        }
      }

      // Send message delta with usage - æ™ºèƒ½å¤„ç†stop_reason
      const hasToolCalls = this.toolCallMap.size > 0;
      const shouldIncludeStopReason = stopReason === 'tool_use' || hasToolCalls;
      
      if (shouldIncludeStopReason) {
        // å·¥å…·è°ƒç”¨å®Œæˆ - éœ€è¦å‘é€stop_reasonä»¥è§¦å‘ä¸‹ä¸€è½®
        const messageDeltaEvent = this.createAnthropicEvent('message_delta', {
          type: 'message_delta',
          delta: { 
            stop_reason: stopReason || 'tool_use',
            stop_sequence: null
          },
          usage: { output_tokens: outputTokens }
        });
        if (messageDeltaEvent) {
          yield messageDeltaEvent;
        }

        // å¼ºåˆ¶è®°å½•å·¥å…·è°ƒç”¨finish reason - æ ¸å¿ƒåŠŸèƒ½ï¼Œå¿…é¡»è®°å½•
        const toolCallFinishReason = stopReason || 'tool_use';
        logger.logFinishReason(toolCallFinishReason, {
          provider: this.options.sourceFormat,
          model: this.model,
          responseType: 'streaming',
          toolCallCount: this.toolCallMap.size,
          context: 'tool-use-completion',
          originalStopReason: stopReason
        }, this.requestId, 'streaming-tool-use');
        
        // åŒæ—¶è®°å½•åˆ°è°ƒè¯•æ—¥å¿—ç³»ç»Ÿ
        try {
          const { logFinishReasonDebug } = await import('../utils/finish-reason-debug');
          logFinishReasonDebug(
            this.requestId,
            toolCallFinishReason,
            this.options.sourceFormat,
            this.model,
            this.options.port || (this as any).port || (() => { console.error('âŒ CRITICAL: Port not provided to streaming transformer'); throw new Error('Port must be provided to streaming transformer - no fallback allowed'); })(),
            {
              toolCallCount: this.toolCallMap.size,
              context: 'tool-use-completion',
              originalStopReason: stopReason,
              timestamp: new Date().toISOString()
            }
          );
        } catch (error) {
          console.error('Failed to log tool call finish reason debug:', error);
        }

        // ğŸ”„ äºŒæ¬¡å·¥å…·è°ƒç”¨å¤„ç†æ£€æŸ¥
        const shouldReprocess = await this.checkForReprocessing(stopReason);
        if (shouldReprocess) {
          const reprocessedResult = await this.reprocessForToolCalls();
          if (reprocessedResult.hasToolCalls) {
            // å‘é€reprocessedå·¥å…·è°ƒç”¨äº‹ä»¶
            for (const toolEvent of reprocessedResult.toolEvents) {
              yield toolEvent;
            }
            // æ›´æ–°åœæ­¢åŸå› 
            stopReason = 'tool_use';
          }
        }

        // åªæœ‰åœ¨étool_useåœºæ™¯æ‰å‘é€message_stopï¼Œå·¥å…·è°ƒç”¨éœ€è¦ä¿æŒå¯¹è¯å¼€æ”¾
        const actualStopReason = stopReason || 'tool_use';
        if (actualStopReason !== 'tool_use') {
          const messageStopEvent = this.createAnthropicEvent('message_stop', {
            type: 'message_stop'
          });
          if (messageStopEvent) {
            yield messageStopEvent;
          }
        }
      } else {
        // éå·¥å…·è°ƒç”¨ - ç§»é™¤stop signalsä¿æŒå¯¹è¯å¼€æ”¾
        const messageDeltaEvent = this.createAnthropicEvent('message_delta', {
          type: 'message_delta',
          delta: { 
            // Empty delta - no stop signals to keep conversation alive
          },
          usage: { output_tokens: outputTokens }
        });
        if (messageDeltaEvent) {
          yield messageDeltaEvent;
        }
        
        // ä¸å‘é€message_stopäº‹ä»¶ï¼Œé¿å…ä¼šè¯ç»ˆæ­¢
      }

    } catch (error) {
      logger.error('Streaming transformation failed', error, this.requestId);
      
      // å³ä½¿åœ¨é”™è¯¯æƒ…å†µä¸‹ä¹Ÿè¦è®°å½•finish reason - æ ¸å¿ƒåŠŸèƒ½ä¸èƒ½ç¼ºå¤±
      try {
        const errorFinishReason = 'error';
        const errorMessage = error instanceof Error ? error.message : 'Unknown error';
        logger.logFinishReason(errorFinishReason, {
          provider: this.options.sourceFormat,
          model: this.model,
          responseType: 'streaming',
          error: errorMessage,
          context: 'streaming-error',
          toolCallCount: this.toolCallMap.size
        }, this.requestId, 'streaming-error');
        
        // åŒæ—¶è®°å½•åˆ°è°ƒè¯•æ—¥å¿—ç³»ç»Ÿ
        try {
          const { logFinishReasonDebug } = await import('../utils/finish-reason-debug');
          logFinishReasonDebug(
            this.requestId,
            errorFinishReason,
            this.options.sourceFormat,
            this.model,
            this.options.port || (this as any).port || (() => { console.error('âŒ CRITICAL: Port not provided to streaming transformer'); throw new Error('Port must be provided to streaming transformer - no fallback allowed'); })(),
            {
              error: errorMessage,
              context: 'streaming-error',
              toolCallCount: this.toolCallMap.size,
              timestamp: new Date().toISOString()
            }
          );
        } catch (debugError) {
          console.error('Failed to log error finish reason debug:', debugError);
        }
      } catch (logError) {
        console.error('Failed to log error finish reason:', logError);
      }
      
      throw error;
    } finally {
      reader.releaseLock();
    }
  }

  /**
   * Transform streaming response from Anthropic to OpenAI format
   */
  async *transformAnthropicToOpenAI(stream: ReadableStream): AsyncIterable<string> {
    const reader = stream.getReader();
    const decoder = new TextDecoder();
    let buffer = '';
    let toolCallIndex = 0;
    let currentToolCalls: Map<number, any> = new Map();

    try {
      while (!this.isCompleted) {
        const { done, value } = await reader.read();
        if (done) break;

        buffer += decoder.decode(value, { stream: true });
        const lines = buffer.split('\n');
        buffer = lines.pop() || '';

        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const data = line.slice(6).trim();
            if (data === '[DONE]') {
              this.isCompleted = true;
              break;
            }

            try {
              const event = JSON.parse(data);
              
              // Handle content block deltas
              if (event.type === 'content_block_delta' && event.delta?.text) {
                // Check for tool calls appearing in text content (error condition)
                this.pipelineDebugger.detectToolCallError(
                  event.delta.text,
                  this.requestId,
                  'streaming-anthropic-to-openai',
                  'anthropic',
                  this.model
                );
                
                yield this.createOpenAIChunk({
                  choices: [{
                    index: 0,
                    delta: { content: event.delta.text }
                  }]
                });
              }

              // Handle tool use blocks
              if (event.type === 'content_block_start' && event.content_block?.type === 'tool_use') {
                const toolCall = {
                  index: toolCallIndex++,
                  id: event.content_block.id,
                  type: 'function',
                  function: {
                    name: event.content_block.name,
                    arguments: ''
                  }
                };
                
                currentToolCalls.set(event.index, toolCall);
                
                yield this.createOpenAIChunk({
                  choices: [{
                    index: 0,
                    delta: { tool_calls: [toolCall] }
                  }]
                });
              }

              // Handle tool input deltas
              if (event.type === 'content_block_delta' && event.delta?.partial_json) {
                const toolCall = currentToolCalls.get(event.index);
                if (toolCall) {
                  yield this.createOpenAIChunk({
                    choices: [{
                      index: 0,
                      delta: {
                        tool_calls: [{
                          index: toolCall.index,
                          function: { arguments: event.delta.partial_json }
                        }]
                      }
                    }]
                  });
                }
              }

              // Handle message completion
              if (event.type === 'message_delta' && event.delta?.stop_reason) {
                const originalStopReason = event.delta.stop_reason;
                const mappedFinishReason = mapStopReason(originalStopReason);
                
                // ğŸ†• è®°å½•åŸå§‹Anthropicå’Œè½¬æ¢åçš„OpenAI finish reason
                logger.logDualFinishReason(
                  originalStopReason,
                  mappedFinishReason,
                  this.options.sourceFormat,
                  {
                    model: this.model,
                    responseType: 'streaming',
                    context: 'streaming-anthropic-to-openai',
                    eventData: event,
                    conversionMethod: 'mapStopReason'
                  },
                  this.requestId,
                  'dual-reason-anthropic-streaming'
                );
                
                // åŒæ—¶è®°å½•åˆ°è°ƒè¯•æ—¥å¿—ç³»ç»Ÿ
                try {
                  const { logStopReasonDebug } = await import('../utils/finish-reason-debug');
                  logStopReasonDebug(
                    this.requestId,
                    originalStopReason,
                    this.options.sourceFormat,
                    this.model,
                    this.options.port || (() => { console.error('âŒ CRITICAL: Port not provided to streaming transformer'); throw new Error('Port must be provided to streaming transformer - no fallback allowed'); })(),
                    {
                      mappedFinishReason,
                      context: 'streaming-transformer-anthropic-dual',
                      timestamp: new Date().toISOString()
                    }
                  );
                } catch (error) {
                  console.error('Failed to log stop reason debug:', error);
                }
                
                yield this.createOpenAIChunk({
                  choices: [{
                    index: 0,
                    delta: {},
                    finish_reason: mappedFinishReason
                  }]
                });
              }

            } catch (error) {
              logger.debug('Failed to parse Anthropic streaming chunk', error, this.requestId);
            }
          }
        }
      }

      // Send final [DONE] marker
      yield 'data: [DONE]\n\n';

    } catch (error) {
      logger.error('Anthropic to OpenAI streaming transformation failed', error, this.requestId);
      
      // å³ä½¿åœ¨é”™è¯¯æƒ…å†µä¸‹ä¹Ÿè¦è®°å½•finish reason - æ ¸å¿ƒåŠŸèƒ½ä¸èƒ½ç¼ºå¤±
      try {
        const errorFinishReason = 'error';
        const errorMessage = error instanceof Error ? error.message : 'Unknown error';
        logger.logFinishReason(errorFinishReason, {
          provider: this.options.sourceFormat,
          model: this.model,
          responseType: 'streaming',
          error: errorMessage,
          context: 'anthropic-streaming-error',
          toolCallCount: this.toolCallMap.size
        }, this.requestId, 'anthropic-streaming-error');
        
        // åŒæ—¶è®°å½•åˆ°è°ƒè¯•æ—¥å¿—ç³»ç»Ÿ
        try {
          const { logFinishReasonDebug } = await import('../utils/finish-reason-debug');
          logFinishReasonDebug(
            this.requestId,
            errorFinishReason,
            this.options.sourceFormat,
            this.model,
            this.options.port || (() => { console.error('âŒ CRITICAL: Port not provided to streaming transformer'); throw new Error('Port must be provided to streaming transformer - no fallback allowed'); })(),
            {
              error: errorMessage,
              context: 'anthropic-streaming-error',
              toolCallCount: this.toolCallMap.size,
              timestamp: new Date().toISOString()
            }
          );
        } catch (debugError) {
          console.error('Failed to log anthropic error finish reason debug:', debugError);
        }
      } catch (logError) {
        console.error('Failed to log anthropic error finish reason:', logError);
      }
      
      throw error;
    } finally {
      reader.releaseLock();
    }
  }

  /**
   * Create Anthropic SSE event - with defensive programming
   */
  private createAnthropicEvent(event: string, data: any): string | null {
    try {
      // Defensive checks like demo1
      if (!event || event === undefined || event === null) {
        return null;
      }
      if (!data || data === undefined || data === null) {
        return null;
      }
      
      const jsonString = JSON.stringify(data);
      if (!jsonString || jsonString === 'null' || jsonString === 'undefined') {
        return null;
      }
      
      return `event: ${event}\ndata: ${jsonString}\n\n`;
    } catch (error) {
      // If JSON.stringify fails, return null instead of undefined
      console.error('Failed to create Anthropic event:', error);
      return null;
    }
  }

  /**
   * Create OpenAI streaming chunk
   */
  private createOpenAIChunk(chunkData: any): string {
    const chunk = {
      id: this.messageId,
      object: 'chat.completion.chunk',
      created: Math.floor(Date.now() / 1000),
      model: this.model,
      ...chunkData
    };
    return `data: ${JSON.stringify(chunk)}\n\n`;
  }

  /**
   * Check if raw data contains tool call signatures
   * Made more strict to avoid false positives from normal text
   */
  private isLikelyToolCallError(rawChunk: string, error: any): boolean {
    // ULTRA STRICT: Only detect actual tool call structures in streaming context
    // Avoid false positives from normal JSON content like {"isNew": false}
    const ultraStrictToolCallPatterns = [
      // Complete tool_use blocks (Anthropic format)
      /\{\s*"type"\s*:\s*"tool_use"\s*,\s*"id"\s*:\s*"[^"]+"\s*,\s*"name"\s*:\s*"[^"]+"/i,
      // Complete function call blocks (OpenAI format)  
      /\{\s*"id"\s*:\s*"call_[a-zA-Z0-9_-]+"\s*,\s*"type"\s*:\s*"function"\s*,\s*"function"/i,
      // Tool calls array context
      /tool_calls":\s*\[\s*\{\s*"id"/i,
      // Function arguments with tool context
      /\{\s*"function"\s*:\s*\{\s*"name"\s*:\s*"[a-zA-Z_][a-zA-Z0-9_]*"\s*,\s*"arguments"/i
    ];

    // Additional context check: only flag if error is parse-related AND contains tool signatures
    const isParseError = error && (
      error.message?.includes('parse') ||
      error.message?.includes('JSON') ||
      error.message?.includes('tool') ||
      error.message?.includes('function')
    );

    // Only trigger if both conditions are met: parse error AND actual tool structure
    return isParseError && ultraStrictToolCallPatterns.some(pattern => pattern.test(rawChunk));
  }

  /**
   * ğŸ”„ æ£€æŸ¥æ˜¯å¦éœ€è¦äºŒæ¬¡å¤„ç† - é”™è¯¯åˆ¤æ–­å·¥å…·è°ƒç”¨çš„æƒ…å†µ
   */
  private async checkForReprocessing(currentStopReason?: string): Promise<boolean> {
    // å¦‚æœå·²ç»å°è¯•è¿‡å¤ªå¤šæ¬¡ï¼Œåœæ­¢reprocessing
    if (this.toolCallDetectionAttempts >= this.maxDetectionAttempts) {
      return false;
    }

    // å¦‚æœå·²ç»æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨ï¼Œé€šå¸¸ä¸éœ€è¦reprocessing
    if (currentStopReason === 'tool_use' && this.toolCallMap.size > 0) {
      return false;
    }

    // æ£€æŸ¥reprocess bufferä¸­æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨ç‰¹å¾
    const bufferContent = this.reprocessBuffer.join(' ');
    const hasToolCallPatterns = this.detectToolCallPatterns(bufferContent);

    // å¦‚æœå½“å‰stop reasonä¸æ˜¯tool_useä½†bufferä¸­æœ‰å·¥å…·è°ƒç”¨æ¨¡å¼ï¼Œéœ€è¦reprocess
    if (currentStopReason !== 'tool_use' && hasToolCallPatterns) {
      this.needsReprocessing = true;
      this.toolCallDetectionAttempts++;
      return true;
    }

    return false;
  }

  /**
   * ğŸ”„ æ‰§è¡ŒäºŒæ¬¡å·¥å…·è°ƒç”¨å¤„ç†
   */
  private async reprocessForToolCalls(): Promise<{
    hasToolCalls: boolean;
    toolEvents: any[];
  }> {
    const toolEvents: any[] = [];
    let hasToolCalls = false;

    try {
      const bufferContent = this.reprocessBuffer.join(' ');
      const extractedTools = this.extractToolCallsFromBuffer(bufferContent);

      if (extractedTools.length > 0) {
        hasToolCalls = true;

        // ä¸ºæ¯ä¸ªæå–çš„å·¥å…·è°ƒç”¨åˆ›å»ºäº‹ä»¶
        for (let i = 0; i < extractedTools.length; i++) {
          const tool = extractedTools[i];
          const blockIndex = this.contentBlockIndex + i;

          // åˆ›å»ºå·¥å…·è°ƒç”¨å¼€å§‹äº‹ä»¶
          const toolStartEvent = this.createAnthropicEvent('content_block_start', {
            type: 'content_block_start',
            index: blockIndex,
            content_block: {
              type: 'tool_use',
              id: tool.id,
              name: tool.name,
              input: {}
            }
          });

          if (toolStartEvent) {
            toolEvents.push(toolStartEvent);
          }

          // åˆ›å»ºå·¥å…·è°ƒç”¨è¾“å…¥äº‹ä»¶
          const toolInputEvent = this.createAnthropicEvent('content_block_delta', {
            type: 'content_block_delta',
            index: blockIndex,
            delta: {
              type: 'input_json_delta',
              partial_json: JSON.stringify(tool.input)
            }
          });

          if (toolInputEvent) {
            toolEvents.push(toolInputEvent);
          }

          // åˆ›å»ºå·¥å…·è°ƒç”¨åœæ­¢äº‹ä»¶
          const toolStopEvent = this.createAnthropicEvent('content_block_stop', {
            type: 'content_block_stop',
            index: blockIndex
          });

          if (toolStopEvent) {
            toolEvents.push(toolStopEvent);
          }

          console.log(`ğŸ”„ [REPROCESS] Extracted tool call: ${tool.name}`);
        }

        this.contentBlockIndex += extractedTools.length;
      }

      // æ¸…ç©ºbuffer
      this.reprocessBuffer = [];

    } catch (error) {
      console.error('ğŸ”„ [REPROCESS] Failed to reprocess for tool calls:', error);
    }

    return { hasToolCalls, toolEvents };
  }

  /**
   * æ£€æµ‹bufferå†…å®¹ä¸­çš„å·¥å…·è°ƒç”¨æ¨¡å¼
   */
  private detectToolCallPatterns(content: string): boolean {
    if (!content || content.trim().length === 0) {
      return false;
    }

    // æ£€æµ‹å„ç§å·¥å…·è°ƒç”¨æ¨¡å¼
    const patterns = [
      /Tool\s+call:\s*\w+\s*\([^)]*\)/i,                    // GLMæ ¼å¼
      /\{\s*"type"\s*:\s*"tool_use"[^}]*\}/i,               // JSONæ ¼å¼
      /\w+\s*\(\s*\{[^}]*"[^"]*"\s*:[^}]*\}/i,             // å‡½æ•°è°ƒç”¨æ ¼å¼
      /"function_call"\s*:\s*\{[^}]*"name"\s*:/i            // OpenAIæ ¼å¼
    ];

    return patterns.some(pattern => pattern.test(content));
  }

  /**
   * ä»bufferä¸­æå–å·¥å…·è°ƒç”¨
   */
  private extractToolCallsFromBuffer(content: string): any[] {
    const tools: any[] = [];

    // GLMæ ¼å¼æå–
    const glmPattern = /Tool\s+call:\s*(\w+)\s*\((\{[^}]*\})\)/gi;
    let match;
    while ((match = glmPattern.exec(content)) !== null) {
      try {
        const toolName = match[1];
        const args = JSON.parse(match[2]);
        tools.push({
          id: `toolu_reprocess_${Date.now()}_${tools.length}`,
          name: toolName,
          input: args
        });
      } catch (error) {
        console.warn('ğŸ”„ [REPROCESS] Failed to parse GLM tool call:', match[0]);
      }
    }

    // JSONæ ¼å¼æå–
    const jsonPattern = /\{\s*"type"\s*:\s*"tool_use"[^}]*\}/gi;
    while ((match = jsonPattern.exec(content)) !== null) {
      try {
        const toolObj = JSON.parse(match[0]);
        if (toolObj.name && toolObj.input) {
          tools.push({
            id: toolObj.id || `toolu_reprocess_json_${Date.now()}_${tools.length}`,
            name: toolObj.name,
            input: toolObj.input
          });
        }
      } catch (error) {
        console.warn('ğŸ”„ [REPROCESS] Failed to parse JSON tool call:', match[0]);
      }
    }

    return tools;
  }

  /**
   * Save raw stream data for analysis
   */
  private saveRawStreamDataForAnalysis(rawStreamData: string[], transformationStage: string, error: any): void {
    try {
      this.pipelineDebugger.addRawStreamData(this.requestId, rawStreamData.join(''));
      
      // Log the error for analysis
      this.pipelineDebugger.logToolCallError(new ToolCallErrorClass(
        `Raw stream analysis error: ${error.message}`,
        this.requestId,
        transformationStage,
        'openai',
        this.model,
        {
          rawChunk: rawStreamData.slice(-5).join(''), // Last 5 chunks for context
        },
        this.options.port || (() => { console.error('âŒ CRITICAL: Port not provided to streaming transformer'); throw new Error('Port must be provided to streaming transformer - no fallback allowed'); })()
      ));
    } catch (analysisError) {
      console.error('Failed to prepare raw stream analysis data:', analysisError);
    }
  }

}

/**
 * Create streaming transformer
 */
export function createStreamingTransformer(
  sourceTransformer: MessageTransformer,
  targetTransformer: MessageTransformer,
  options: StreamTransformOptions
): StreamingTransformer {
  return new StreamingTransformer(sourceTransformer, targetTransformer, options);
}