#!/usr/bin/env node

/**
 * ç®€å•çš„è½¬æ¢æµ‹è¯•ï¼šæ¨¡æ‹Ÿ OpenAI -> Unified -> Anthropic æµç¨‹
 */

// ç®€å•çš„æ˜ å°„å‡½æ•°ï¼ˆå¤åˆ¶è‡ªæˆ‘ä»¬çš„ä»£ç ï¼‰
function mapOpenAIFinishReason(finishReason) {
  const mapping = {
    'stop': 'end_turn',
    'length': 'max_tokens',
    'function_call': 'tool_use',
    'tool_calls': 'tool_use',
    'content_filter': 'stop_sequence'
  };
  return mapping[finishReason] || 'end_turn';
}

// æ¨¡æ‹ŸOpenAI response -> Unified responseè½¬æ¢
function mockOpenAIToUnified(openaiResponse) {
  const choice = openaiResponse.choices[0];
  return {
    id: openaiResponse.id,
    object: 'chat.completion',
    created: openaiResponse.created,
    model: openaiResponse.model,
    choices: [{
      index: 0,
      message: {
        role: choice.message.role,
        content: choice.message.content,
        tool_calls: choice.message.tool_calls
      },
      finish_reason: choice.finish_reason
    }],
    usage: openaiResponse.usage
  };
}

// æ¨¡æ‹ŸUnified -> Anthropicè½¬æ¢ï¼ˆåŸºäºæˆ‘ä»¬ä¿®å¤çš„ä»£ç ï¼‰
function mockUnifiedToAnthropic(unifiedResponse) {
  const choice = unifiedResponse.choices[0];
  const content = [];

  // æ·»åŠ æ–‡æœ¬å†…å®¹
  if (choice.message.content) {
    content.push({
      type: 'text',
      text: choice.message.content
    });
  }

  // æ·»åŠ å·¥å…·è°ƒç”¨
  if (choice.message.tool_calls) {
    choice.message.tool_calls.forEach(toolCall => {
      content.push({
        type: 'tool_use',
        id: toolCall.id,
        name: toolCall.function.name,
        input: JSON.parse(toolCall.function.arguments || '{}')
      });
    });
  }

  // å…³é”®ï¼šåº”ç”¨æˆ‘ä»¬çš„ä¿®å¤
  return {
    id: unifiedResponse.id,
    type: 'message',
    role: 'assistant',
    content,
    model: unifiedResponse.model,
    stop_reason: mapOpenAIFinishReason(choice.finish_reason || 'stop'),
    stop_sequence: null,
    usage: {
      input_tokens: unifiedResponse.usage.prompt_tokens,
      output_tokens: unifiedResponse.usage.completion_tokens
    }
  };
}

// æµ‹è¯•ç”¨ä¾‹
const testCases = [
  {
    name: "æ­£å¸¸ç»“æŸ",
    openaiResponse: {
      id: "test-1",
      object: "chat.completion",
      created: 1234567890,
      model: "qwen3-coder",
      choices: [{
        index: 0,
        message: {
          role: "assistant",
          content: "Hello! How can I help you?"
        },
        finish_reason: "stop"
      }],
      usage: { prompt_tokens: 10, completion_tokens: 8, total_tokens: 18 }
    }
  },
  {
    name: "å·¥å…·è°ƒç”¨",
    openaiResponse: {
      id: "test-2",
      object: "chat.completion", 
      created: 1234567890,
      model: "qwen3-coder",
      choices: [{
        index: 0,
        message: {
          role: "assistant",
          content: "I'll edit the file.",
          tool_calls: [{
            id: "call_123",
            type: "function",
            function: {
              name: "Edit",
              arguments: '{"file_path": "/tmp/test.txt", "old_string": "", "new_string": "hello"}'
            }
          }]
        },
        finish_reason: "tool_calls"
      }],
      usage: { prompt_tokens: 50, completion_tokens: 20, total_tokens: 70 }
    }
  },
  {
    name: "è¾¾åˆ°é•¿åº¦é™åˆ¶",
    openaiResponse: {
      id: "test-3",
      object: "chat.completion",
      created: 1234567890, 
      model: "qwen3-coder",
      choices: [{
        index: 0,
        message: {
          role: "assistant",
          content: "This response was cut off..."
        },
        finish_reason: "length"
      }],
      usage: { prompt_tokens: 20, completion_tokens: 100, total_tokens: 120 }
    }
  }
];

console.log('ğŸ§ª æ¨¡æ‹Ÿå®Œæ•´è½¬æ¢ç®¡é“æµ‹è¯•');
console.log('='.repeat(50));

testCases.forEach((testCase, index) => {
  console.log(`\nğŸ“‹ æµ‹è¯• ${index + 1}: ${testCase.name}`);
  console.log('-'.repeat(30));
  
  try {
    const originalFinishReason = testCase.openaiResponse.choices[0].finish_reason;
    console.log(`ğŸ”¤ åŸå§‹ finish_reason: "${originalFinishReason}"`);
    
    // ç¬¬ä¸€æ­¥ï¼šOpenAI -> Unified
    const unified = mockOpenAIToUnified(testCase.openaiResponse);
    console.log(`ğŸ”„ Unified finish_reason: "${unified.choices[0].finish_reason}"`);
    
    // ç¬¬äºŒæ­¥ï¼šUnified -> Anthropic
    const anthropic = mockUnifiedToAnthropic(unified);
    console.log(`ğŸ¯ Anthropic stop_reason: "${anthropic.stop_reason}"`);
    
    // éªŒè¯æ˜ å°„
    const expectedStopReason = mapOpenAIFinishReason(originalFinishReason);
    if (anthropic.stop_reason === expectedStopReason) {
      console.log(`âœ… æ˜ å°„æ­£ç¡®: ${originalFinishReason} -> ${anthropic.stop_reason}`);
    } else {
      console.log(`âŒ æ˜ å°„é”™è¯¯: ${originalFinishReason} -> ${anthropic.stop_reason} (æœŸæœ›: ${expectedStopReason})`);
    }
    
    // æ˜¾ç¤ºå®Œæ•´ç»“æ„
    console.log(`ğŸ“¦ æœ€ç»ˆç»“æ„: id=${anthropic.id}, type=${anthropic.type}, blocks=${anthropic.content.length}`);
    
  } catch (error) {
    console.log(`âŒ æµ‹è¯•å¤±è´¥: ${error.message}`);
  }
});

console.log(`\n${'='.repeat(50)}`);
console.log('ğŸ æ¨¡æ‹Ÿæµ‹è¯•å®Œæˆ - ç†è®ºä¸Šè½¬æ¢åº”è¯¥å·¥ä½œæ­£å¸¸ï¼');
console.log('ğŸ’¡ å¦‚æœçœŸå®APIä»ç„¶è¿”å›nullï¼Œé—®é¢˜å¯èƒ½åœ¨äºï¼š');
console.log('   1. OpenAI Provideræœªæ­£ç¡®ä¼ é€’finish_reason');  
console.log('   2. Anthropic Output Processorè¦†ç›–äº†stop_reason');
console.log('   3. å…¶ä»–ä¸­é—´å¤„ç†æ­¥éª¤ä¸¢å¤±äº†å­—æ®µ');