{
  "server": {
    "port": 5506,
    "host": "localhost"
  },
  "standardProviders": {
    "lmstudio": {
      "name": "LM Studio",
      "protocol": "openai",
      "connection": {
        "endpoint": "http://localhost:1234/v1/chat/completions"
      },
      "priority": 100
    }
  },
  "serverCompatibilityProviders": {},
  "configVersion": "4.0"
}